{
    "run_name": "run_20_r16_alpha32_lr3e-05_optpaged_adamw_8bit_ep10",
    "r": 16,
    "lora_alpha": 32,
    "optim": "paged_adamw_8bit",
    "learning_rate": 3e-05,
    "lr_scheduler": "cosine",
    "weight_decay": 0.01,
    "num_train_epochs": 10,
    "train_log": [
        {
            "loss": 3.4544,
            "grad_norm": 1.7290894985198975,
            "learning_rate": 2.9985318397799606e-05,
            "epoch": 0.14084507042253522,
            "step": 10
        },
        {
            "loss": 3.3676,
            "grad_norm": 1.549971103668213,
            "learning_rate": 2.9941302331124173e-05,
            "epoch": 0.28169014084507044,
            "step": 20
        },
        {
            "loss": 3.4382,
            "grad_norm": 2.190330743789673,
            "learning_rate": 2.986803796349122e-05,
            "epoch": 0.4225352112676056,
            "step": 30
        },
        {
            "loss": 2.9118,
            "grad_norm": 2.0610060691833496,
            "learning_rate": 2.9765668713340883e-05,
            "epoch": 0.5633802816901409,
            "step": 40
        },
        {
            "loss": 2.8923,
            "grad_norm": 3.055098056793213,
            "learning_rate": 2.9634394973287605e-05,
            "epoch": 0.704225352112676,
            "step": 50
        },
        {
            "loss": 2.9342,
            "grad_norm": 3.0300676822662354,
            "learning_rate": 2.947447371784215e-05,
            "epoch": 0.8450704225352113,
            "step": 60
        },
        {
            "loss": 3.0634,
            "grad_norm": 3.3729658126831055,
            "learning_rate": 2.9286218000371973e-05,
            "epoch": 0.9859154929577465,
            "step": 70
        },
        {
            "loss": 2.9314,
            "grad_norm": 3.94355845451355,
            "learning_rate": 2.9069996340284513e-05,
            "epoch": 1.1267605633802817,
            "step": 80
        },
        {
            "loss": 2.5412,
            "grad_norm": 4.530173301696777,
            "learning_rate": 2.8826232001633174e-05,
            "epoch": 1.267605633802817,
            "step": 90
        },
        {
            "loss": 2.4889,
            "grad_norm": 5.122811794281006,
            "learning_rate": 2.8555402164558058e-05,
            "epoch": 1.408450704225352,
            "step": 100
        },
        {
            "loss": 2.4626,
            "grad_norm": 4.893174171447754,
            "learning_rate": 2.8258036991183414e-05,
            "epoch": 1.5492957746478875,
            "step": 110
        },
        {
            "loss": 2.396,
            "grad_norm": 6.424295902252197,
            "learning_rate": 2.793471858780042e-05,
            "epoch": 1.6901408450704225,
            "step": 120
        },
        {
            "loss": 2.1192,
            "grad_norm": 5.663955211639404,
            "learning_rate": 2.758607986536673e-05,
            "epoch": 1.8309859154929577,
            "step": 130
        },
        {
            "loss": 2.2389,
            "grad_norm": 7.745055198669434,
            "learning_rate": 2.721280330055362e-05,
            "epoch": 1.971830985915493,
            "step": 140
        },
        {
            "loss": 2.1664,
            "grad_norm": 7.389334201812744,
            "learning_rate": 2.6815619599765775e-05,
            "epoch": 2.112676056338028,
            "step": 150
        },
        {
            "loss": 1.928,
            "grad_norm": 8.588133811950684,
            "learning_rate": 2.6395306268749274e-05,
            "epoch": 2.2535211267605635,
            "step": 160
        },
        {
            "loss": 1.8038,
            "grad_norm": 9.083775520324707,
            "learning_rate": 2.5952686090587515e-05,
            "epoch": 2.3943661971830985,
            "step": 170
        },
        {
            "loss": 1.6404,
            "grad_norm": 8.906559944152832,
            "learning_rate": 2.5488625515064713e-05,
            "epoch": 2.535211267605634,
            "step": 180
        },
        {
            "loss": 1.7427,
            "grad_norm": 6.918807506561279,
            "learning_rate": 2.500403296254976e-05,
            "epoch": 2.676056338028169,
            "step": 190
        },
        {
            "loss": 1.6967,
            "grad_norm": 9.010369300842285,
            "learning_rate": 2.4499857045720705e-05,
            "epoch": 2.816901408450704,
            "step": 200
        },
        {
            "loss": 1.5618,
            "grad_norm": 7.631948471069336,
            "learning_rate": 2.3977084712610862e-05,
            "epoch": 2.9577464788732395,
            "step": 210
        },
        {
            "loss": 1.5623,
            "grad_norm": 13.634021759033203,
            "learning_rate": 2.343673931461171e-05,
            "epoch": 3.0985915492957745,
            "step": 220
        },
        {
            "loss": 1.2767,
            "grad_norm": 7.708117961883545,
            "learning_rate": 2.287987860321448e-05,
            "epoch": 3.23943661971831,
            "step": 230
        },
        {
            "loss": 1.2975,
            "grad_norm": 8.220161437988281,
            "learning_rate": 2.230759265941195e-05,
            "epoch": 3.380281690140845,
            "step": 240
        },
        {
            "loss": 1.1676,
            "grad_norm": 8.655893325805664,
            "learning_rate": 2.1721001759813677e-05,
            "epoch": 3.52112676056338,
            "step": 250
        },
        {
            "loss": 1.1606,
            "grad_norm": 10.521346092224121,
            "learning_rate": 2.1121254183651974e-05,
            "epoch": 3.6619718309859155,
            "step": 260
        },
        {
            "loss": 1.0763,
            "grad_norm": 7.935743808746338,
            "learning_rate": 2.0509523964971355e-05,
            "epoch": 3.802816901408451,
            "step": 270
        },
        {
            "loss": 1.1765,
            "grad_norm": 10.718560218811035,
            "learning_rate": 1.9887008594401765e-05,
            "epoch": 3.943661971830986,
            "step": 280
        },
        {
            "loss": 1.022,
            "grad_norm": 16.40111541748047,
            "learning_rate": 1.9254926675014452e-05,
            "epoch": 4.084507042253521,
            "step": 290
        },
        {
            "loss": 0.8849,
            "grad_norm": 9.38262939453125,
            "learning_rate": 1.8614515536849215e-05,
            "epoch": 4.225352112676056,
            "step": 300
        },
        {
            "loss": 0.8074,
            "grad_norm": 12.09374713897705,
            "learning_rate": 1.796702881478276e-05,
            "epoch": 4.366197183098592,
            "step": 310
        },
        {
            "loss": 0.7987,
            "grad_norm": 10.883757591247559,
            "learning_rate": 1.7313733994479534e-05,
            "epoch": 4.507042253521127,
            "step": 320
        },
        {
            "loss": 0.7703,
            "grad_norm": 9.371221542358398,
            "learning_rate": 1.665590993122905e-05,
            "epoch": 4.647887323943662,
            "step": 330
        },
        {
            "loss": 0.871,
            "grad_norm": 9.830138206481934,
            "learning_rate": 1.599484434652656e-05,
            "epoch": 4.788732394366197,
            "step": 340
        },
        {
            "loss": 0.7888,
            "grad_norm": 15.423996925354004,
            "learning_rate": 1.5331831307297803e-05,
            "epoch": 4.929577464788732,
            "step": 350
        },
        {
            "loss": 0.7039,
            "grad_norm": 6.854852676391602,
            "learning_rate": 1.46681686927022e-05,
            "epoch": 5.070422535211268,
            "step": 360
        },
        {
            "loss": 0.5401,
            "grad_norm": 8.07239818572998,
            "learning_rate": 1.4005155653473445e-05,
            "epoch": 5.211267605633803,
            "step": 370
        },
        {
            "loss": 0.5528,
            "grad_norm": 7.683756351470947,
            "learning_rate": 1.3344090068770957e-05,
            "epoch": 5.352112676056338,
            "step": 380
        },
        {
            "loss": 0.5473,
            "grad_norm": 8.787388801574707,
            "learning_rate": 1.2686266005520462e-05,
            "epoch": 5.492957746478873,
            "step": 390
        },
        {
            "loss": 0.5317,
            "grad_norm": 13.988073348999023,
            "learning_rate": 1.2032971185217241e-05,
            "epoch": 5.633802816901408,
            "step": 400
        },
        {
            "loss": 0.546,
            "grad_norm": 8.92714786529541,
            "learning_rate": 1.1385484463150784e-05,
            "epoch": 5.774647887323944,
            "step": 410
        },
        {
            "loss": 0.5803,
            "grad_norm": 12.898862838745117,
            "learning_rate": 1.074507332498555e-05,
            "epoch": 5.915492957746479,
            "step": 420
        },
        {
            "loss": 0.531,
            "grad_norm": 6.750102519989014,
            "learning_rate": 1.0112991405598239e-05,
            "epoch": 6.056338028169014,
            "step": 430
        },
        {
            "loss": 0.3509,
            "grad_norm": 8.149822235107422,
            "learning_rate": 9.490476035028652e-06,
            "epoch": 6.197183098591549,
            "step": 440
        },
        {
            "loss": 0.4803,
            "grad_norm": 8.123964309692383,
            "learning_rate": 8.878745816348025e-06,
            "epoch": 6.338028169014084,
            "step": 450
        },
        {
            "loss": 0.354,
            "grad_norm": 8.232390403747559,
            "learning_rate": 8.278998240186322e-06,
            "epoch": 6.47887323943662,
            "step": 460
        },
        {
            "loss": 0.4634,
            "grad_norm": 6.496568202972412,
            "learning_rate": 7.692407340588055e-06,
            "epoch": 6.619718309859155,
            "step": 470
        },
        {
            "loss": 0.3972,
            "grad_norm": 9.191018104553223,
            "learning_rate": 7.120121396785521e-06,
            "epoch": 6.76056338028169,
            "step": 480
        },
        {
            "loss": 0.4536,
            "grad_norm": 7.139076232910156,
            "learning_rate": 6.563260685388291e-06,
            "epoch": 6.901408450704225,
            "step": 490
        },
        {
            "loss": 0.4078,
            "grad_norm": 5.905939102172852,
            "learning_rate": 6.02291528738914e-06,
            "epoch": 7.042253521126761,
            "step": 500
        },
        {
            "loss": 0.3082,
            "grad_norm": 7.573357582092285,
            "learning_rate": 5.500142954279293e-06,
            "epoch": 7.183098591549296,
            "step": 510
        },
        {
            "loss": 0.3791,
            "grad_norm": 4.459411144256592,
            "learning_rate": 4.995967037450238e-06,
            "epoch": 7.323943661971831,
            "step": 520
        },
        {
            "loss": 0.3035,
            "grad_norm": 7.535919666290283,
            "learning_rate": 4.5113744849352894e-06,
            "epoch": 7.464788732394366,
            "step": 530
        },
        {
            "loss": 0.337,
            "grad_norm": 5.571537971496582,
            "learning_rate": 4.047313909412488e-06,
            "epoch": 7.605633802816901,
            "step": 540
        },
        {
            "loss": 0.3337,
            "grad_norm": 6.226034164428711,
            "learning_rate": 3.6046937312507296e-06,
            "epoch": 7.746478873239437,
            "step": 550
        },
        {
            "loss": 0.3306,
            "grad_norm": 6.372344017028809,
            "learning_rate": 3.1843804002342296e-06,
            "epoch": 7.887323943661972,
            "step": 560
        },
        {
            "loss": 0.2929,
            "grad_norm": 6.59699821472168,
            "learning_rate": 2.7871966994463887e-06,
            "epoch": 8.028169014084508,
            "step": 570
        },
        {
            "loss": 0.2725,
            "grad_norm": 5.756268501281738,
            "learning_rate": 2.413920134633272e-06,
            "epoch": 8.169014084507042,
            "step": 580
        },
        {
            "loss": 0.2487,
            "grad_norm": 4.557657241821289,
            "learning_rate": 2.0652814121995824e-06,
            "epoch": 8.309859154929578,
            "step": 590
        },
        {
            "loss": 0.2956,
            "grad_norm": 7.7503437995910645,
            "learning_rate": 1.7419630088165832e-06,
            "epoch": 8.450704225352112,
            "step": 600
        },
        {
            "loss": 0.3199,
            "grad_norm": 4.901457786560059,
            "learning_rate": 1.4445978354419437e-06,
            "epoch": 8.591549295774648,
            "step": 610
        },
        {
            "loss": 0.3126,
            "grad_norm": 9.987386703491211,
            "learning_rate": 1.1737679983668259e-06,
            "epoch": 8.732394366197184,
            "step": 620
        },
        {
            "loss": 0.339,
            "grad_norm": 6.525285720825195,
            "learning_rate": 9.300036597154881e-07,
            "epoch": 8.873239436619718,
            "step": 630
        },
        {
            "loss": 0.2641,
            "grad_norm": 4.800474643707275,
            "learning_rate": 7.137819996280303e-07,
            "epoch": 9.014084507042254,
            "step": 640
        },
        {
            "loss": 0.2761,
            "grad_norm": 4.079979419708252,
            "learning_rate": 5.255262821578521e-07,
            "epoch": 9.154929577464788,
            "step": 650
        },
        {
            "loss": 0.2945,
            "grad_norm": 5.165215969085693,
            "learning_rate": 3.656050267123984e-07,
            "epoch": 9.295774647887324,
            "step": 660
        },
        {
            "loss": 0.2878,
            "grad_norm": 8.073321342468262,
            "learning_rate": 2.343312866591163e-07,
            "epoch": 9.43661971830986,
            "step": 670
        },
        {
            "loss": 0.2502,
            "grad_norm": 3.086468458175659,
            "learning_rate": 1.3196203650878148e-07,
            "epoch": 9.577464788732394,
            "step": 680
        },
        {
            "loss": 0.3108,
            "grad_norm": 5.411575794219971,
            "learning_rate": 5.869766887582784e-08,
            "epoch": 9.71830985915493,
            "step": 690
        },
        {
            "loss": 0.2565,
            "grad_norm": 4.618438243865967,
            "learning_rate": 1.4681602200395938e-08,
            "epoch": 9.859154929577464,
            "step": 700
        },
        {
            "loss": 0.273,
            "grad_norm": 4.059145450592041,
            "learning_rate": 0.0,
            "epoch": 10.0,
            "step": 710
        },
        {
            "train_runtime": 6202.8982,
            "train_samples_per_second": 1.83,
            "train_steps_per_second": 0.114,
            "total_flos": 2.632119817863168e+17,
            "train_loss": 1.1291187259512887,
            "epoch": 10.0,
            "step": 710
        },
        {
            "eval_loss": 2.4807441234588623,
            "eval_runtime": 52.2396,
            "eval_samples_per_second": 5.436,
            "eval_steps_per_second": 0.689,
            "epoch": 10.0,
            "step": 710
        }
    ],
    "perplexity": 11.9501534997079
}