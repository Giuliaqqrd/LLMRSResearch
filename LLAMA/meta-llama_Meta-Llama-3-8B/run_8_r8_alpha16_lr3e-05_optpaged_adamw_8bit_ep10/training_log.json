{
    "run_name": "run_8_r8_alpha16_lr3e-05_optpaged_adamw_8bit_ep10",
    "r": 8,
    "lora_alpha": 16,
    "optim": "paged_adamw_8bit",
    "learning_rate": 3e-05,
    "lr_scheduler": "cosine",
    "weight_decay": 0.01,
    "num_train_epochs": 10,
    "train_log": [
        {
            "loss": 3.4767,
            "grad_norm": 1.1910018920898438,
            "learning_rate": 2.9985318397799606e-05,
            "epoch": 0.14084507042253522,
            "step": 10
        },
        {
            "loss": 3.4386,
            "grad_norm": 1.2237615585327148,
            "learning_rate": 2.9941302331124173e-05,
            "epoch": 0.28169014084507044,
            "step": 20
        },
        {
            "loss": 3.5729,
            "grad_norm": 1.7092065811157227,
            "learning_rate": 2.986803796349122e-05,
            "epoch": 0.4225352112676056,
            "step": 30
        },
        {
            "loss": 3.0514,
            "grad_norm": 1.632279396057129,
            "learning_rate": 2.9765668713340883e-05,
            "epoch": 0.5633802816901409,
            "step": 40
        },
        {
            "loss": 3.028,
            "grad_norm": 2.305490732192993,
            "learning_rate": 2.9634394973287605e-05,
            "epoch": 0.704225352112676,
            "step": 50
        },
        {
            "loss": 3.094,
            "grad_norm": 2.1828856468200684,
            "learning_rate": 2.947447371784215e-05,
            "epoch": 0.8450704225352113,
            "step": 60
        },
        {
            "loss": 3.2312,
            "grad_norm": 2.8150482177734375,
            "learning_rate": 2.9286218000371973e-05,
            "epoch": 0.9859154929577465,
            "step": 70
        },
        {
            "loss": 3.1551,
            "grad_norm": 3.1795947551727295,
            "learning_rate": 2.9069996340284513e-05,
            "epoch": 1.1267605633802817,
            "step": 80
        },
        {
            "loss": 2.7899,
            "grad_norm": 3.5357255935668945,
            "learning_rate": 2.8826232001633174e-05,
            "epoch": 1.267605633802817,
            "step": 90
        },
        {
            "loss": 2.739,
            "grad_norm": 3.9414615631103516,
            "learning_rate": 2.8555402164558058e-05,
            "epoch": 1.408450704225352,
            "step": 100
        },
        {
            "loss": 2.6967,
            "grad_norm": 3.606135845184326,
            "learning_rate": 2.8258036991183414e-05,
            "epoch": 1.5492957746478875,
            "step": 110
        },
        {
            "loss": 2.6608,
            "grad_norm": 4.8219523429870605,
            "learning_rate": 2.793471858780042e-05,
            "epoch": 1.6901408450704225,
            "step": 120
        },
        {
            "loss": 2.3719,
            "grad_norm": 4.161089897155762,
            "learning_rate": 2.758607986536673e-05,
            "epoch": 1.8309859154929577,
            "step": 130
        },
        {
            "loss": 2.4865,
            "grad_norm": 5.558123588562012,
            "learning_rate": 2.721280330055362e-05,
            "epoch": 1.971830985915493,
            "step": 140
        },
        {
            "loss": 2.5408,
            "grad_norm": 5.4911112785339355,
            "learning_rate": 2.6815619599765775e-05,
            "epoch": 2.112676056338028,
            "step": 150
        },
        {
            "loss": 2.3072,
            "grad_norm": 6.733535289764404,
            "learning_rate": 2.6395306268749274e-05,
            "epoch": 2.2535211267605635,
            "step": 160
        },
        {
            "loss": 2.1958,
            "grad_norm": 6.916457653045654,
            "learning_rate": 2.5952686090587515e-05,
            "epoch": 2.3943661971830985,
            "step": 170
        },
        {
            "loss": 2.0413,
            "grad_norm": 7.5365214347839355,
            "learning_rate": 2.5488625515064713e-05,
            "epoch": 2.535211267605634,
            "step": 180
        },
        {
            "loss": 2.1186,
            "grad_norm": 5.415126323699951,
            "learning_rate": 2.500403296254976e-05,
            "epoch": 2.676056338028169,
            "step": 190
        },
        {
            "loss": 2.0841,
            "grad_norm": 7.356611728668213,
            "learning_rate": 2.4499857045720705e-05,
            "epoch": 2.816901408450704,
            "step": 200
        },
        {
            "loss": 1.9905,
            "grad_norm": 6.367656230926514,
            "learning_rate": 2.3977084712610862e-05,
            "epoch": 2.9577464788732395,
            "step": 210
        },
        {
            "loss": 2.0442,
            "grad_norm": 9.956342697143555,
            "learning_rate": 2.343673931461171e-05,
            "epoch": 3.0985915492957745,
            "step": 220
        },
        {
            "loss": 1.743,
            "grad_norm": 7.016963481903076,
            "learning_rate": 2.287987860321448e-05,
            "epoch": 3.23943661971831,
            "step": 230
        },
        {
            "loss": 1.7541,
            "grad_norm": 6.805479526519775,
            "learning_rate": 2.230759265941195e-05,
            "epoch": 3.380281690140845,
            "step": 240
        },
        {
            "loss": 1.5966,
            "grad_norm": 8.40374755859375,
            "learning_rate": 2.1721001759813677e-05,
            "epoch": 3.52112676056338,
            "step": 250
        },
        {
            "loss": 1.5818,
            "grad_norm": 9.883220672607422,
            "learning_rate": 2.1121254183651974e-05,
            "epoch": 3.6619718309859155,
            "step": 260
        },
        {
            "loss": 1.4846,
            "grad_norm": 7.8990936279296875,
            "learning_rate": 2.0509523964971355e-05,
            "epoch": 3.802816901408451,
            "step": 270
        },
        {
            "loss": 1.5899,
            "grad_norm": 9.83156967163086,
            "learning_rate": 1.9887008594401765e-05,
            "epoch": 3.943661971830986,
            "step": 280
        },
        {
            "loss": 1.5077,
            "grad_norm": 13.616925239562988,
            "learning_rate": 1.9254926675014452e-05,
            "epoch": 4.084507042253521,
            "step": 290
        },
        {
            "loss": 1.318,
            "grad_norm": 8.459603309631348,
            "learning_rate": 1.8614515536849215e-05,
            "epoch": 4.225352112676056,
            "step": 300
        },
        {
            "loss": 1.2558,
            "grad_norm": 12.443943977355957,
            "learning_rate": 1.796702881478276e-05,
            "epoch": 4.366197183098592,
            "step": 310
        },
        {
            "loss": 1.191,
            "grad_norm": 11.596810340881348,
            "learning_rate": 1.7313733994479534e-05,
            "epoch": 4.507042253521127,
            "step": 320
        },
        {
            "loss": 1.1454,
            "grad_norm": 9.222355842590332,
            "learning_rate": 1.665590993122905e-05,
            "epoch": 4.647887323943662,
            "step": 330
        },
        {
            "loss": 1.3109,
            "grad_norm": 9.095091819763184,
            "learning_rate": 1.599484434652656e-05,
            "epoch": 4.788732394366197,
            "step": 340
        },
        {
            "loss": 1.2026,
            "grad_norm": 13.722369194030762,
            "learning_rate": 1.5331831307297803e-05,
            "epoch": 4.929577464788732,
            "step": 350
        },
        {
            "loss": 1.104,
            "grad_norm": 9.228191375732422,
            "learning_rate": 1.46681686927022e-05,
            "epoch": 5.070422535211268,
            "step": 360
        },
        {
            "loss": 0.9049,
            "grad_norm": 9.491609573364258,
            "learning_rate": 1.4005155653473445e-05,
            "epoch": 5.211267605633803,
            "step": 370
        },
        {
            "loss": 0.9731,
            "grad_norm": 8.776001930236816,
            "learning_rate": 1.3344090068770957e-05,
            "epoch": 5.352112676056338,
            "step": 380
        },
        {
            "loss": 0.9493,
            "grad_norm": 10.904631614685059,
            "learning_rate": 1.2686266005520462e-05,
            "epoch": 5.492957746478873,
            "step": 390
        },
        {
            "loss": 0.9066,
            "grad_norm": 14.327737808227539,
            "learning_rate": 1.2032971185217241e-05,
            "epoch": 5.633802816901408,
            "step": 400
        },
        {
            "loss": 0.8794,
            "grad_norm": 10.334364891052246,
            "learning_rate": 1.1385484463150784e-05,
            "epoch": 5.774647887323944,
            "step": 410
        },
        {
            "loss": 0.9564,
            "grad_norm": 15.702207565307617,
            "learning_rate": 1.074507332498555e-05,
            "epoch": 5.915492957746479,
            "step": 420
        },
        {
            "loss": 0.9163,
            "grad_norm": 8.76620864868164,
            "learning_rate": 1.0112991405598239e-05,
            "epoch": 6.056338028169014,
            "step": 430
        },
        {
            "loss": 0.6983,
            "grad_norm": 11.62951946258545,
            "learning_rate": 9.490476035028652e-06,
            "epoch": 6.197183098591549,
            "step": 440
        },
        {
            "loss": 0.8406,
            "grad_norm": 9.000308990478516,
            "learning_rate": 8.878745816348025e-06,
            "epoch": 6.338028169014084,
            "step": 450
        },
        {
            "loss": 0.6563,
            "grad_norm": 10.918217658996582,
            "learning_rate": 8.278998240186322e-06,
            "epoch": 6.47887323943662,
            "step": 460
        },
        {
            "loss": 0.7176,
            "grad_norm": 8.001479148864746,
            "learning_rate": 7.692407340588055e-06,
            "epoch": 6.619718309859155,
            "step": 470
        },
        {
            "loss": 0.708,
            "grad_norm": 12.256854057312012,
            "learning_rate": 7.120121396785521e-06,
            "epoch": 6.76056338028169,
            "step": 480
        },
        {
            "loss": 0.7497,
            "grad_norm": 10.915613174438477,
            "learning_rate": 6.563260685388291e-06,
            "epoch": 6.901408450704225,
            "step": 490
        },
        {
            "loss": 0.7291,
            "grad_norm": 9.17380142211914,
            "learning_rate": 6.02291528738914e-06,
            "epoch": 7.042253521126761,
            "step": 500
        },
        {
            "loss": 0.542,
            "grad_norm": 10.340301513671875,
            "learning_rate": 5.500142954279293e-06,
            "epoch": 7.183098591549296,
            "step": 510
        },
        {
            "loss": 0.6467,
            "grad_norm": 7.495426177978516,
            "learning_rate": 4.995967037450238e-06,
            "epoch": 7.323943661971831,
            "step": 520
        },
        {
            "loss": 0.5672,
            "grad_norm": 12.849803924560547,
            "learning_rate": 4.5113744849352894e-06,
            "epoch": 7.464788732394366,
            "step": 530
        },
        {
            "loss": 0.5975,
            "grad_norm": 8.157330513000488,
            "learning_rate": 4.047313909412488e-06,
            "epoch": 7.605633802816901,
            "step": 540
        },
        {
            "loss": 0.6181,
            "grad_norm": 10.444993019104004,
            "learning_rate": 3.6046937312507296e-06,
            "epoch": 7.746478873239437,
            "step": 550
        },
        {
            "loss": 0.5975,
            "grad_norm": 11.115739822387695,
            "learning_rate": 3.1843804002342296e-06,
            "epoch": 7.887323943661972,
            "step": 560
        },
        {
            "loss": 0.5196,
            "grad_norm": 12.001039505004883,
            "learning_rate": 2.7871966994463887e-06,
            "epoch": 8.028169014084508,
            "step": 570
        },
        {
            "loss": 0.505,
            "grad_norm": 9.6390380859375,
            "learning_rate": 2.413920134633272e-06,
            "epoch": 8.169014084507042,
            "step": 580
        },
        {
            "loss": 0.4628,
            "grad_norm": 8.526837348937988,
            "learning_rate": 2.0652814121995824e-06,
            "epoch": 8.309859154929578,
            "step": 590
        },
        {
            "loss": 0.5642,
            "grad_norm": 12.068775177001953,
            "learning_rate": 1.7419630088165832e-06,
            "epoch": 8.450704225352112,
            "step": 600
        },
        {
            "loss": 0.5564,
            "grad_norm": 8.549901008605957,
            "learning_rate": 1.4445978354419437e-06,
            "epoch": 8.591549295774648,
            "step": 610
        },
        {
            "loss": 0.6162,
            "grad_norm": 12.635589599609375,
            "learning_rate": 1.1737679983668259e-06,
            "epoch": 8.732394366197184,
            "step": 620
        },
        {
            "loss": 0.5765,
            "grad_norm": 11.28579330444336,
            "learning_rate": 9.300036597154881e-07,
            "epoch": 8.873239436619718,
            "step": 630
        },
        {
            "loss": 0.4445,
            "grad_norm": 8.35831069946289,
            "learning_rate": 7.137819996280303e-07,
            "epoch": 9.014084507042254,
            "step": 640
        },
        {
            "loss": 0.4889,
            "grad_norm": 7.0508623123168945,
            "learning_rate": 5.255262821578521e-07,
            "epoch": 9.154929577464788,
            "step": 650
        },
        {
            "loss": 0.4944,
            "grad_norm": 8.454341888427734,
            "learning_rate": 3.656050267123984e-07,
            "epoch": 9.295774647887324,
            "step": 660
        },
        {
            "loss": 0.521,
            "grad_norm": 8.696065902709961,
            "learning_rate": 2.343312866591163e-07,
            "epoch": 9.43661971830986,
            "step": 670
        },
        {
            "loss": 0.4605,
            "grad_norm": 7.289634704589844,
            "learning_rate": 1.3196203650878148e-07,
            "epoch": 9.577464788732394,
            "step": 680
        },
        {
            "loss": 0.5627,
            "grad_norm": 10.338451385498047,
            "learning_rate": 5.869766887582784e-08,
            "epoch": 9.71830985915493,
            "step": 690
        },
        {
            "loss": 0.5014,
            "grad_norm": 9.866765022277832,
            "learning_rate": 1.4681602200395938e-08,
            "epoch": 9.859154929577464,
            "step": 700
        },
        {
            "loss": 0.4787,
            "grad_norm": 7.074071884155273,
            "learning_rate": 0.0,
            "epoch": 10.0,
            "step": 710
        },
        {
            "train_runtime": 6203.6815,
            "train_samples_per_second": 1.83,
            "train_steps_per_second": 0.114,
            "total_flos": 2.624438456549376e+17,
            "train_loss": 1.4339146768543083,
            "epoch": 10.0,
            "step": 710
        },
        {
            "eval_loss": 2.411726713180542,
            "eval_runtime": 52.312,
            "eval_samples_per_second": 5.429,
            "eval_steps_per_second": 0.688,
            "epoch": 10.0,
            "step": 710
        }
    ],
    "perplexity": 11.153202910250073
}