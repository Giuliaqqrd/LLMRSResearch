{
    "run_name": "run_15_r16_alpha32_lr5e-05_optpaged_adamw_8bit_ep10",
    "r": 16,
    "lora_alpha": 32,
    "optim": "paged_adamw_8bit",
    "learning_rate": 5e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.01,
    "num_train_epochs": 10,
    "train_log": [
        {
            "loss": 3.4214,
            "grad_norm": 1.860421895980835,
            "learning_rate": 4.929577464788733e-05,
            "epoch": 0.14084507042253522,
            "step": 10
        },
        {
            "loss": 3.2627,
            "grad_norm": 1.7811074256896973,
            "learning_rate": 4.8591549295774653e-05,
            "epoch": 0.28169014084507044,
            "step": 20
        },
        {
            "loss": 3.2684,
            "grad_norm": 2.3206841945648193,
            "learning_rate": 4.788732394366197e-05,
            "epoch": 0.4225352112676056,
            "step": 30
        },
        {
            "loss": 2.7694,
            "grad_norm": 2.130718946456909,
            "learning_rate": 4.71830985915493e-05,
            "epoch": 0.5633802816901409,
            "step": 40
        },
        {
            "loss": 2.7525,
            "grad_norm": 3.2845396995544434,
            "learning_rate": 4.647887323943662e-05,
            "epoch": 0.704225352112676,
            "step": 50
        },
        {
            "loss": 2.769,
            "grad_norm": 3.6769189834594727,
            "learning_rate": 4.577464788732395e-05,
            "epoch": 0.8450704225352113,
            "step": 60
        },
        {
            "loss": 2.8867,
            "grad_norm": 3.857353687286377,
            "learning_rate": 4.507042253521127e-05,
            "epoch": 0.9859154929577465,
            "step": 70
        },
        {
            "loss": 2.6563,
            "grad_norm": 4.939501762390137,
            "learning_rate": 4.436619718309859e-05,
            "epoch": 1.1267605633802817,
            "step": 80
        },
        {
            "loss": 2.2346,
            "grad_norm": 5.241732597351074,
            "learning_rate": 4.366197183098591e-05,
            "epoch": 1.267605633802817,
            "step": 90
        },
        {
            "loss": 2.182,
            "grad_norm": 5.89657735824585,
            "learning_rate": 4.295774647887324e-05,
            "epoch": 1.408450704225352,
            "step": 100
        },
        {
            "loss": 2.1757,
            "grad_norm": 5.961023807525635,
            "learning_rate": 4.225352112676056e-05,
            "epoch": 1.5492957746478875,
            "step": 110
        },
        {
            "loss": 2.0695,
            "grad_norm": 7.362566947937012,
            "learning_rate": 4.154929577464789e-05,
            "epoch": 1.6901408450704225,
            "step": 120
        },
        {
            "loss": 1.8185,
            "grad_norm": 6.197293758392334,
            "learning_rate": 4.0845070422535214e-05,
            "epoch": 1.8309859154929577,
            "step": 130
        },
        {
            "loss": 1.9618,
            "grad_norm": 8.14789867401123,
            "learning_rate": 4.014084507042254e-05,
            "epoch": 1.971830985915493,
            "step": 140
        },
        {
            "loss": 1.7091,
            "grad_norm": 8.309896469116211,
            "learning_rate": 3.943661971830986e-05,
            "epoch": 2.112676056338028,
            "step": 150
        },
        {
            "loss": 1.4938,
            "grad_norm": 8.682896614074707,
            "learning_rate": 3.8732394366197184e-05,
            "epoch": 2.2535211267605635,
            "step": 160
        },
        {
            "loss": 1.3958,
            "grad_norm": 9.704036712646484,
            "learning_rate": 3.802816901408451e-05,
            "epoch": 2.3943661971830985,
            "step": 170
        },
        {
            "loss": 1.232,
            "grad_norm": 8.204476356506348,
            "learning_rate": 3.7323943661971835e-05,
            "epoch": 2.535211267605634,
            "step": 180
        },
        {
            "loss": 1.3177,
            "grad_norm": 6.801482677459717,
            "learning_rate": 3.661971830985916e-05,
            "epoch": 2.676056338028169,
            "step": 190
        },
        {
            "loss": 1.3048,
            "grad_norm": 8.6961030960083,
            "learning_rate": 3.5915492957746486e-05,
            "epoch": 2.816901408450704,
            "step": 200
        },
        {
            "loss": 1.199,
            "grad_norm": 6.455721378326416,
            "learning_rate": 3.5211267605633805e-05,
            "epoch": 2.9577464788732395,
            "step": 210
        },
        {
            "loss": 1.1513,
            "grad_norm": 11.88931941986084,
            "learning_rate": 3.450704225352113e-05,
            "epoch": 3.0985915492957745,
            "step": 220
        },
        {
            "loss": 0.8558,
            "grad_norm": 7.8059492111206055,
            "learning_rate": 3.380281690140845e-05,
            "epoch": 3.23943661971831,
            "step": 230
        },
        {
            "loss": 0.8804,
            "grad_norm": 6.904111385345459,
            "learning_rate": 3.3098591549295775e-05,
            "epoch": 3.380281690140845,
            "step": 240
        },
        {
            "loss": 0.7975,
            "grad_norm": 9.55778980255127,
            "learning_rate": 3.23943661971831e-05,
            "epoch": 3.52112676056338,
            "step": 250
        },
        {
            "loss": 0.767,
            "grad_norm": 9.051132202148438,
            "learning_rate": 3.1690140845070426e-05,
            "epoch": 3.6619718309859155,
            "step": 260
        },
        {
            "loss": 0.7453,
            "grad_norm": 7.467757701873779,
            "learning_rate": 3.0985915492957744e-05,
            "epoch": 3.802816901408451,
            "step": 270
        },
        {
            "loss": 0.8379,
            "grad_norm": 9.546777725219727,
            "learning_rate": 3.028169014084507e-05,
            "epoch": 3.943661971830986,
            "step": 280
        },
        {
            "loss": 0.6581,
            "grad_norm": 11.27547836303711,
            "learning_rate": 2.9577464788732395e-05,
            "epoch": 4.084507042253521,
            "step": 290
        },
        {
            "loss": 0.5789,
            "grad_norm": 8.740715980529785,
            "learning_rate": 2.887323943661972e-05,
            "epoch": 4.225352112676056,
            "step": 300
        },
        {
            "loss": 0.5285,
            "grad_norm": 8.646525382995605,
            "learning_rate": 2.8169014084507046e-05,
            "epoch": 4.366197183098592,
            "step": 310
        },
        {
            "loss": 0.5286,
            "grad_norm": 8.436126708984375,
            "learning_rate": 2.746478873239437e-05,
            "epoch": 4.507042253521127,
            "step": 320
        },
        {
            "loss": 0.5179,
            "grad_norm": 7.487380504608154,
            "learning_rate": 2.676056338028169e-05,
            "epoch": 4.647887323943662,
            "step": 330
        },
        {
            "loss": 0.5778,
            "grad_norm": 7.1867828369140625,
            "learning_rate": 2.6056338028169013e-05,
            "epoch": 4.788732394366197,
            "step": 340
        },
        {
            "loss": 0.5214,
            "grad_norm": 11.137165069580078,
            "learning_rate": 2.535211267605634e-05,
            "epoch": 4.929577464788732,
            "step": 350
        },
        {
            "loss": 0.4521,
            "grad_norm": 3.8861215114593506,
            "learning_rate": 2.4647887323943664e-05,
            "epoch": 5.070422535211268,
            "step": 360
        },
        {
            "loss": 0.3679,
            "grad_norm": 5.622506141662598,
            "learning_rate": 2.3943661971830986e-05,
            "epoch": 5.211267605633803,
            "step": 370
        },
        {
            "loss": 0.35,
            "grad_norm": 5.377438068389893,
            "learning_rate": 2.323943661971831e-05,
            "epoch": 5.352112676056338,
            "step": 380
        },
        {
            "loss": 0.3973,
            "grad_norm": 5.730709075927734,
            "learning_rate": 2.2535211267605634e-05,
            "epoch": 5.492957746478873,
            "step": 390
        },
        {
            "loss": 0.3777,
            "grad_norm": 7.237889766693115,
            "learning_rate": 2.1830985915492956e-05,
            "epoch": 5.633802816901408,
            "step": 400
        },
        {
            "loss": 0.3845,
            "grad_norm": 6.625843048095703,
            "learning_rate": 2.112676056338028e-05,
            "epoch": 5.774647887323944,
            "step": 410
        },
        {
            "loss": 0.4223,
            "grad_norm": 9.422025680541992,
            "learning_rate": 2.0422535211267607e-05,
            "epoch": 5.915492957746479,
            "step": 420
        },
        {
            "loss": 0.3704,
            "grad_norm": 5.009961128234863,
            "learning_rate": 1.971830985915493e-05,
            "epoch": 6.056338028169014,
            "step": 430
        },
        {
            "loss": 0.2568,
            "grad_norm": 4.438813209533691,
            "learning_rate": 1.9014084507042255e-05,
            "epoch": 6.197183098591549,
            "step": 440
        },
        {
            "loss": 0.3554,
            "grad_norm": 5.018646240234375,
            "learning_rate": 1.830985915492958e-05,
            "epoch": 6.338028169014084,
            "step": 450
        },
        {
            "loss": 0.2621,
            "grad_norm": 5.2978997230529785,
            "learning_rate": 1.7605633802816902e-05,
            "epoch": 6.47887323943662,
            "step": 460
        },
        {
            "loss": 0.3651,
            "grad_norm": 6.117378234863281,
            "learning_rate": 1.6901408450704224e-05,
            "epoch": 6.619718309859155,
            "step": 470
        },
        {
            "loss": 0.3018,
            "grad_norm": 5.93674898147583,
            "learning_rate": 1.619718309859155e-05,
            "epoch": 6.76056338028169,
            "step": 480
        },
        {
            "loss": 0.3505,
            "grad_norm": 5.11893367767334,
            "learning_rate": 1.5492957746478872e-05,
            "epoch": 6.901408450704225,
            "step": 490
        },
        {
            "loss": 0.3223,
            "grad_norm": 2.9689366817474365,
            "learning_rate": 1.4788732394366198e-05,
            "epoch": 7.042253521126761,
            "step": 500
        },
        {
            "loss": 0.2515,
            "grad_norm": 4.122630596160889,
            "learning_rate": 1.4084507042253523e-05,
            "epoch": 7.183098591549296,
            "step": 510
        },
        {
            "loss": 0.2864,
            "grad_norm": 4.412753105163574,
            "learning_rate": 1.3380281690140845e-05,
            "epoch": 7.323943661971831,
            "step": 520
        },
        {
            "loss": 0.2628,
            "grad_norm": 4.68990421295166,
            "learning_rate": 1.267605633802817e-05,
            "epoch": 7.464788732394366,
            "step": 530
        },
        {
            "loss": 0.2762,
            "grad_norm": 4.4448723793029785,
            "learning_rate": 1.1971830985915493e-05,
            "epoch": 7.605633802816901,
            "step": 540
        },
        {
            "loss": 0.2711,
            "grad_norm": 3.9671685695648193,
            "learning_rate": 1.1267605633802817e-05,
            "epoch": 7.746478873239437,
            "step": 550
        },
        {
            "loss": 0.293,
            "grad_norm": 6.052433967590332,
            "learning_rate": 1.056338028169014e-05,
            "epoch": 7.887323943661972,
            "step": 560
        },
        {
            "loss": 0.251,
            "grad_norm": 4.534695625305176,
            "learning_rate": 9.859154929577465e-06,
            "epoch": 8.028169014084508,
            "step": 570
        },
        {
            "loss": 0.2159,
            "grad_norm": 5.647481441497803,
            "learning_rate": 9.15492957746479e-06,
            "epoch": 8.169014084507042,
            "step": 580
        },
        {
            "loss": 0.2272,
            "grad_norm": 6.20368766784668,
            "learning_rate": 8.450704225352112e-06,
            "epoch": 8.309859154929578,
            "step": 590
        },
        {
            "loss": 0.2407,
            "grad_norm": 4.691342353820801,
            "learning_rate": 7.746478873239436e-06,
            "epoch": 8.450704225352112,
            "step": 600
        },
        {
            "loss": 0.2746,
            "grad_norm": 2.8888163566589355,
            "learning_rate": 7.042253521126762e-06,
            "epoch": 8.591549295774648,
            "step": 610
        },
        {
            "loss": 0.2307,
            "grad_norm": 5.674998760223389,
            "learning_rate": 6.338028169014085e-06,
            "epoch": 8.732394366197184,
            "step": 620
        },
        {
            "loss": 0.2781,
            "grad_norm": 4.400972843170166,
            "learning_rate": 5.6338028169014084e-06,
            "epoch": 8.873239436619718,
            "step": 630
        },
        {
            "loss": 0.2559,
            "grad_norm": 2.747185468673706,
            "learning_rate": 4.929577464788732e-06,
            "epoch": 9.014084507042254,
            "step": 640
        },
        {
            "loss": 0.2422,
            "grad_norm": 2.5802228450775146,
            "learning_rate": 4.225352112676056e-06,
            "epoch": 9.154929577464788,
            "step": 650
        },
        {
            "loss": 0.2285,
            "grad_norm": 3.3182032108306885,
            "learning_rate": 3.521126760563381e-06,
            "epoch": 9.295774647887324,
            "step": 660
        },
        {
            "loss": 0.2079,
            "grad_norm": 3.153749704360962,
            "learning_rate": 2.8169014084507042e-06,
            "epoch": 9.43661971830986,
            "step": 670
        },
        {
            "loss": 0.212,
            "grad_norm": 2.247436285018921,
            "learning_rate": 2.112676056338028e-06,
            "epoch": 9.577464788732394,
            "step": 680
        },
        {
            "loss": 0.2561,
            "grad_norm": 3.0981900691986084,
            "learning_rate": 1.4084507042253521e-06,
            "epoch": 9.71830985915493,
            "step": 690
        },
        {
            "loss": 0.2177,
            "grad_norm": 2.828906774520874,
            "learning_rate": 7.042253521126761e-07,
            "epoch": 9.859154929577464,
            "step": 700
        },
        {
            "loss": 0.2275,
            "grad_norm": 2.288269519805908,
            "learning_rate": 0.0,
            "epoch": 10.0,
            "step": 710
        },
        {
            "train_runtime": 6195.6161,
            "train_samples_per_second": 1.832,
            "train_steps_per_second": 0.115,
            "total_flos": 2.632119817863168e+17,
            "train_loss": 0.9347950760747346,
            "epoch": 10.0,
            "step": 710
        },
        {
            "eval_loss": 2.553805112838745,
            "eval_runtime": 52.242,
            "eval_samples_per_second": 5.436,
            "eval_steps_per_second": 0.689,
            "epoch": 10.0,
            "step": 710
        }
    ],
    "perplexity": 12.85592909168738
}