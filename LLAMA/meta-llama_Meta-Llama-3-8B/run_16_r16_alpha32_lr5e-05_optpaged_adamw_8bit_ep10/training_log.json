{
    "run_name": "run_16_r16_alpha32_lr5e-05_optpaged_adamw_8bit_ep10",
    "r": 16,
    "lora_alpha": 32,
    "optim": "paged_adamw_8bit",
    "learning_rate": 5e-05,
    "lr_scheduler": "cosine",
    "weight_decay": 0.01,
    "num_train_epochs": 10,
    "train_log": [
        {
            "loss": 3.4198,
            "grad_norm": 1.9342525005340576,
            "learning_rate": 4.9975530662999344e-05,
            "epoch": 0.14084507042253522,
            "step": 10
        },
        {
            "loss": 3.2561,
            "grad_norm": 1.8524540662765503,
            "learning_rate": 4.990217055187362e-05,
            "epoch": 0.28169014084507044,
            "step": 20
        },
        {
            "loss": 3.2584,
            "grad_norm": 2.3694443702697754,
            "learning_rate": 4.978006327248537e-05,
            "epoch": 0.4225352112676056,
            "step": 30
        },
        {
            "loss": 2.761,
            "grad_norm": 2.236337423324585,
            "learning_rate": 4.960944785556814e-05,
            "epoch": 0.5633802816901409,
            "step": 40
        },
        {
            "loss": 2.7394,
            "grad_norm": 3.460921049118042,
            "learning_rate": 4.9390658288812675e-05,
            "epoch": 0.704225352112676,
            "step": 50
        },
        {
            "loss": 2.7533,
            "grad_norm": 3.8304145336151123,
            "learning_rate": 4.9124122863070255e-05,
            "epoch": 0.8450704225352113,
            "step": 60
        },
        {
            "loss": 2.8697,
            "grad_norm": 3.9540770053863525,
            "learning_rate": 4.881036333395329e-05,
            "epoch": 0.9859154929577465,
            "step": 70
        },
        {
            "loss": 2.6235,
            "grad_norm": 5.019546985626221,
            "learning_rate": 4.8449993900474187e-05,
            "epoch": 1.1267605633802817,
            "step": 80
        },
        {
            "loss": 2.194,
            "grad_norm": 5.339295387268066,
            "learning_rate": 4.804372000272196e-05,
            "epoch": 1.267605633802817,
            "step": 90
        },
        {
            "loss": 2.1356,
            "grad_norm": 6.14255428314209,
            "learning_rate": 4.75923369409301e-05,
            "epoch": 1.408450704225352,
            "step": 100
        },
        {
            "loss": 2.1381,
            "grad_norm": 6.110201358795166,
            "learning_rate": 4.7096728318639025e-05,
            "epoch": 1.5492957746478875,
            "step": 110
        },
        {
            "loss": 2.0152,
            "grad_norm": 7.748345851898193,
            "learning_rate": 4.6557864313000695e-05,
            "epoch": 1.6901408450704225,
            "step": 120
        },
        {
            "loss": 1.7751,
            "grad_norm": 6.22265625,
            "learning_rate": 4.597679977561122e-05,
            "epoch": 1.8309859154929577,
            "step": 130
        },
        {
            "loss": 1.9218,
            "grad_norm": 8.076848030090332,
            "learning_rate": 4.535467216758936e-05,
            "epoch": 1.971830985915493,
            "step": 140
        },
        {
            "loss": 1.6298,
            "grad_norm": 8.503936767578125,
            "learning_rate": 4.469269933294296e-05,
            "epoch": 2.112676056338028,
            "step": 150
        },
        {
            "loss": 1.4144,
            "grad_norm": 8.698420524597168,
            "learning_rate": 4.3992177114582124e-05,
            "epoch": 2.2535211267605635,
            "step": 160
        },
        {
            "loss": 1.3314,
            "grad_norm": 9.769553184509277,
            "learning_rate": 4.325447681764586e-05,
            "epoch": 2.3943661971830985,
            "step": 170
        },
        {
            "loss": 1.1586,
            "grad_norm": 8.422572135925293,
            "learning_rate": 4.2481042525107854e-05,
            "epoch": 2.535211267605634,
            "step": 180
        },
        {
            "loss": 1.2594,
            "grad_norm": 6.730059623718262,
            "learning_rate": 4.167338827091627e-05,
            "epoch": 2.676056338028169,
            "step": 190
        },
        {
            "loss": 1.245,
            "grad_norm": 8.87677001953125,
            "learning_rate": 4.083309507620118e-05,
            "epoch": 2.816901408450704,
            "step": 200
        },
        {
            "loss": 1.1522,
            "grad_norm": 6.017223358154297,
            "learning_rate": 3.996180785435144e-05,
            "epoch": 2.9577464788732395,
            "step": 210
        },
        {
            "loss": 1.0724,
            "grad_norm": 11.085644721984863,
            "learning_rate": 3.906123219101952e-05,
            "epoch": 3.0985915492957745,
            "step": 220
        },
        {
            "loss": 0.7738,
            "grad_norm": 7.379328727722168,
            "learning_rate": 3.813313100535747e-05,
            "epoch": 3.23943661971831,
            "step": 230
        },
        {
            "loss": 0.8078,
            "grad_norm": 5.634807586669922,
            "learning_rate": 3.7179321099019916e-05,
            "epoch": 3.380281690140845,
            "step": 240
        },
        {
            "loss": 0.7294,
            "grad_norm": 8.514009475708008,
            "learning_rate": 3.6201669599689465e-05,
            "epoch": 3.52112676056338,
            "step": 250
        },
        {
            "loss": 0.7096,
            "grad_norm": 8.745702743530273,
            "learning_rate": 3.520209030608662e-05,
            "epoch": 3.6619718309859155,
            "step": 260
        },
        {
            "loss": 0.6943,
            "grad_norm": 6.834640979766846,
            "learning_rate": 3.418253994161892e-05,
            "epoch": 3.802816901408451,
            "step": 270
        },
        {
            "loss": 0.7751,
            "grad_norm": 9.860433578491211,
            "learning_rate": 3.3145014324002944e-05,
            "epoch": 3.943661971830986,
            "step": 280
        },
        {
            "loss": 0.6149,
            "grad_norm": 10.498628616333008,
            "learning_rate": 3.209154445835742e-05,
            "epoch": 4.084507042253521,
            "step": 290
        },
        {
            "loss": 0.5231,
            "grad_norm": 7.7907185554504395,
            "learning_rate": 3.102419256141536e-05,
            "epoch": 4.225352112676056,
            "step": 300
        },
        {
            "loss": 0.4815,
            "grad_norm": 6.9579176902771,
            "learning_rate": 2.9945048024637935e-05,
            "epoch": 4.366197183098592,
            "step": 310
        },
        {
            "loss": 0.4863,
            "grad_norm": 8.039623260498047,
            "learning_rate": 2.885622332413256e-05,
            "epoch": 4.507042253521127,
            "step": 320
        },
        {
            "loss": 0.4754,
            "grad_norm": 6.748277187347412,
            "learning_rate": 2.775984988538175e-05,
            "epoch": 4.647887323943662,
            "step": 330
        },
        {
            "loss": 0.5342,
            "grad_norm": 6.236854553222656,
            "learning_rate": 2.6658073910877603e-05,
            "epoch": 4.788732394366197,
            "step": 340
        },
        {
            "loss": 0.481,
            "grad_norm": 12.031042098999023,
            "learning_rate": 2.555305217882967e-05,
            "epoch": 4.929577464788732,
            "step": 350
        },
        {
            "loss": 0.4265,
            "grad_norm": 3.6010384559631348,
            "learning_rate": 2.444694782117033e-05,
            "epoch": 5.070422535211268,
            "step": 360
        },
        {
            "loss": 0.3442,
            "grad_norm": 6.784946441650391,
            "learning_rate": 2.334192608912241e-05,
            "epoch": 5.211267605633803,
            "step": 370
        },
        {
            "loss": 0.324,
            "grad_norm": 4.509854793548584,
            "learning_rate": 2.224015011461826e-05,
            "epoch": 5.352112676056338,
            "step": 380
        },
        {
            "loss": 0.3662,
            "grad_norm": 5.071412563323975,
            "learning_rate": 2.114377667586744e-05,
            "epoch": 5.492957746478873,
            "step": 390
        },
        {
            "loss": 0.3531,
            "grad_norm": 6.292269706726074,
            "learning_rate": 2.0054951975362067e-05,
            "epoch": 5.633802816901408,
            "step": 400
        },
        {
            "loss": 0.3443,
            "grad_norm": 5.616702556610107,
            "learning_rate": 1.8975807438584642e-05,
            "epoch": 5.774647887323944,
            "step": 410
        },
        {
            "loss": 0.3859,
            "grad_norm": 10.236230850219727,
            "learning_rate": 1.7908455541642584e-05,
            "epoch": 5.915492957746479,
            "step": 420
        },
        {
            "loss": 0.3452,
            "grad_norm": 5.437107563018799,
            "learning_rate": 1.6854985675997066e-05,
            "epoch": 6.056338028169014,
            "step": 430
        },
        {
            "loss": 0.2422,
            "grad_norm": 3.46726655960083,
            "learning_rate": 1.5817460058381088e-05,
            "epoch": 6.197183098591549,
            "step": 440
        },
        {
            "loss": 0.3151,
            "grad_norm": 4.6547675132751465,
            "learning_rate": 1.4797909693913376e-05,
            "epoch": 6.338028169014084,
            "step": 450
        },
        {
            "loss": 0.2422,
            "grad_norm": 4.538673400878906,
            "learning_rate": 1.3798330400310539e-05,
            "epoch": 6.47887323943662,
            "step": 460
        },
        {
            "loss": 0.3314,
            "grad_norm": 4.158585548400879,
            "learning_rate": 1.2820678900980093e-05,
            "epoch": 6.619718309859155,
            "step": 470
        },
        {
            "loss": 0.2829,
            "grad_norm": 5.362114429473877,
            "learning_rate": 1.1866868994642535e-05,
            "epoch": 6.76056338028169,
            "step": 480
        },
        {
            "loss": 0.3083,
            "grad_norm": 5.18438196182251,
            "learning_rate": 1.0938767808980486e-05,
            "epoch": 6.901408450704225,
            "step": 490
        },
        {
            "loss": 0.2892,
            "grad_norm": 2.9982731342315674,
            "learning_rate": 1.0038192145648567e-05,
            "epoch": 7.042253521126761,
            "step": 500
        },
        {
            "loss": 0.2408,
            "grad_norm": 3.0580174922943115,
            "learning_rate": 9.166904923798821e-06,
            "epoch": 7.183098591549296,
            "step": 510
        },
        {
            "loss": 0.2515,
            "grad_norm": 5.011102199554443,
            "learning_rate": 8.32661172908373e-06,
            "epoch": 7.323943661971831,
            "step": 520
        },
        {
            "loss": 0.2442,
            "grad_norm": 5.1814045906066895,
            "learning_rate": 7.518957474892149e-06,
            "epoch": 7.464788732394366,
            "step": 530
        },
        {
            "loss": 0.2529,
            "grad_norm": 4.468425273895264,
            "learning_rate": 6.745523182354147e-06,
            "epoch": 7.605633802816901,
            "step": 540
        },
        {
            "loss": 0.2572,
            "grad_norm": 3.7146217823028564,
            "learning_rate": 6.007822885417882e-06,
            "epoch": 7.746478873239437,
            "step": 550
        },
        {
            "loss": 0.2661,
            "grad_norm": 3.5084033012390137,
            "learning_rate": 5.307300667057049e-06,
            "epoch": 7.887323943661972,
            "step": 560
        },
        {
            "loss": 0.2344,
            "grad_norm": 4.428173065185547,
            "learning_rate": 4.645327832410648e-06,
            "epoch": 8.028169014084508,
            "step": 570
        },
        {
            "loss": 0.1996,
            "grad_norm": 3.4882009029388428,
            "learning_rate": 4.023200224388787e-06,
            "epoch": 8.169014084507042,
            "step": 580
        },
        {
            "loss": 0.2116,
            "grad_norm": 2.392444372177124,
            "learning_rate": 3.4421356869993037e-06,
            "epoch": 8.309859154929578,
            "step": 590
        },
        {
            "loss": 0.2221,
            "grad_norm": 3.4728119373321533,
            "learning_rate": 2.9032716813609723e-06,
            "epoch": 8.450704225352112,
            "step": 600
        },
        {
            "loss": 0.2564,
            "grad_norm": 2.856052875518799,
            "learning_rate": 2.4076630590699062e-06,
            "epoch": 8.591549295774648,
            "step": 610
        },
        {
            "loss": 0.2167,
            "grad_norm": 4.306769371032715,
            "learning_rate": 1.956279997278043e-06,
            "epoch": 8.732394366197184,
            "step": 620
        },
        {
            "loss": 0.254,
            "grad_norm": 4.196390628814697,
            "learning_rate": 1.5500060995258137e-06,
            "epoch": 8.873239436619718,
            "step": 630
        },
        {
            "loss": 0.2338,
            "grad_norm": 2.9031574726104736,
            "learning_rate": 1.1896366660467173e-06,
            "epoch": 9.014084507042254,
            "step": 640
        },
        {
            "loss": 0.2372,
            "grad_norm": 2.4473555088043213,
            "learning_rate": 8.758771369297536e-07,
            "epoch": 9.154929577464788,
            "step": 650
        },
        {
            "loss": 0.2224,
            "grad_norm": 3.0808568000793457,
            "learning_rate": 6.093417111873306e-07,
            "epoch": 9.295774647887324,
            "step": 660
        },
        {
            "loss": 0.2013,
            "grad_norm": 2.6016547679901123,
            "learning_rate": 3.905521444318605e-07,
            "epoch": 9.43661971830986,
            "step": 670
        },
        {
            "loss": 0.2034,
            "grad_norm": 1.9985158443450928,
            "learning_rate": 2.1993672751463579e-07,
            "epoch": 9.577464788732394,
            "step": 680
        },
        {
            "loss": 0.2523,
            "grad_norm": 3.0763485431671143,
            "learning_rate": 9.782944812637973e-08,
            "epoch": 9.71830985915493,
            "step": 690
        },
        {
            "loss": 0.2081,
            "grad_norm": 2.6807613372802734,
            "learning_rate": 2.44693370006599e-08,
            "epoch": 9.859154929577464,
            "step": 700
        },
        {
            "loss": 0.2158,
            "grad_norm": 2.1708738803863525,
            "learning_rate": 0.0,
            "epoch": 10.0,
            "step": 710
        },
        {
            "train_runtime": 6201.9416,
            "train_samples_per_second": 1.83,
            "train_steps_per_second": 0.114,
            "total_flos": 2.632119817863168e+17,
            "train_loss": 0.9013151561710196,
            "epoch": 10.0,
            "step": 710
        },
        {
            "eval_loss": 2.458169460296631,
            "eval_runtime": 52.2607,
            "eval_samples_per_second": 5.434,
            "eval_steps_per_second": 0.689,
            "epoch": 10.0,
            "step": 710
        }
    ],
    "perplexity": 11.683405016459625
}