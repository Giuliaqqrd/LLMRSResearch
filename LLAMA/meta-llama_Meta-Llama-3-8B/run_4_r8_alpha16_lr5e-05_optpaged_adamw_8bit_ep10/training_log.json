{
    "run_name": "run_4_r8_alpha16_lr5e-05_optpaged_adamw_8bit_ep10",
    "r": 8,
    "lora_alpha": 16,
    "optim": "paged_adamw_8bit",
    "learning_rate": 5e-05,
    "lr_scheduler": "cosine",
    "weight_decay": 0.01,
    "num_train_epochs": 10,
    "train_log": [
        {
            "loss": 3.4595,
            "grad_norm": 1.3070634603500366,
            "learning_rate": 4.9975530662999344e-05,
            "epoch": 0.14084507042253522,
            "step": 10
        },
        {
            "loss": 3.364,
            "grad_norm": 1.362196683883667,
            "learning_rate": 4.990217055187362e-05,
            "epoch": 0.28169014084507044,
            "step": 20
        },
        {
            "loss": 3.4162,
            "grad_norm": 1.986438512802124,
            "learning_rate": 4.978006327248537e-05,
            "epoch": 0.4225352112676056,
            "step": 30
        },
        {
            "loss": 2.889,
            "grad_norm": 1.8046585321426392,
            "learning_rate": 4.960944785556814e-05,
            "epoch": 0.5633802816901409,
            "step": 40
        },
        {
            "loss": 2.8687,
            "grad_norm": 2.867499828338623,
            "learning_rate": 4.9390658288812675e-05,
            "epoch": 0.704225352112676,
            "step": 50
        },
        {
            "loss": 2.9031,
            "grad_norm": 2.793114185333252,
            "learning_rate": 4.9124122863070255e-05,
            "epoch": 0.8450704225352113,
            "step": 60
        },
        {
            "loss": 3.0252,
            "grad_norm": 3.148545026779175,
            "learning_rate": 4.881036333395329e-05,
            "epoch": 0.9859154929577465,
            "step": 70
        },
        {
            "loss": 2.8797,
            "grad_norm": 3.779282331466675,
            "learning_rate": 4.8449993900474187e-05,
            "epoch": 1.1267605633802817,
            "step": 80
        },
        {
            "loss": 2.4729,
            "grad_norm": 4.441894054412842,
            "learning_rate": 4.804372000272196e-05,
            "epoch": 1.267605633802817,
            "step": 90
        },
        {
            "loss": 2.4172,
            "grad_norm": 5.0630412101745605,
            "learning_rate": 4.75923369409301e-05,
            "epoch": 1.408450704225352,
            "step": 100
        },
        {
            "loss": 2.3959,
            "grad_norm": 4.868541717529297,
            "learning_rate": 4.7096728318639025e-05,
            "epoch": 1.5492957746478875,
            "step": 110
        },
        {
            "loss": 2.3186,
            "grad_norm": 6.38506555557251,
            "learning_rate": 4.6557864313000695e-05,
            "epoch": 1.6901408450704225,
            "step": 120
        },
        {
            "loss": 2.0404,
            "grad_norm": 5.749117374420166,
            "learning_rate": 4.597679977561122e-05,
            "epoch": 1.8309859154929577,
            "step": 130
        },
        {
            "loss": 2.1642,
            "grad_norm": 7.5781450271606445,
            "learning_rate": 4.535467216758936e-05,
            "epoch": 1.971830985915493,
            "step": 140
        },
        {
            "loss": 2.0222,
            "grad_norm": 7.553125858306885,
            "learning_rate": 4.469269933294296e-05,
            "epoch": 2.112676056338028,
            "step": 150
        },
        {
            "loss": 1.7862,
            "grad_norm": 8.634589195251465,
            "learning_rate": 4.3992177114582124e-05,
            "epoch": 2.2535211267605635,
            "step": 160
        },
        {
            "loss": 1.6669,
            "grad_norm": 9.266587257385254,
            "learning_rate": 4.325447681764586e-05,
            "epoch": 2.3943661971830985,
            "step": 170
        },
        {
            "loss": 1.5012,
            "grad_norm": 8.537689208984375,
            "learning_rate": 4.2481042525107854e-05,
            "epoch": 2.535211267605634,
            "step": 180
        },
        {
            "loss": 1.5876,
            "grad_norm": 6.70070743560791,
            "learning_rate": 4.167338827091627e-05,
            "epoch": 2.676056338028169,
            "step": 190
        },
        {
            "loss": 1.5497,
            "grad_norm": 8.399740219116211,
            "learning_rate": 4.083309507620118e-05,
            "epoch": 2.816901408450704,
            "step": 200
        },
        {
            "loss": 1.4251,
            "grad_norm": 7.0387282371521,
            "learning_rate": 3.996180785435144e-05,
            "epoch": 2.9577464788732395,
            "step": 210
        },
        {
            "loss": 1.3945,
            "grad_norm": 12.21023178100586,
            "learning_rate": 3.906123219101952e-05,
            "epoch": 3.0985915492957745,
            "step": 220
        },
        {
            "loss": 1.1038,
            "grad_norm": 6.956499099731445,
            "learning_rate": 3.813313100535747e-05,
            "epoch": 3.23943661971831,
            "step": 230
        },
        {
            "loss": 1.1327,
            "grad_norm": 8.72323989868164,
            "learning_rate": 3.7179321099019916e-05,
            "epoch": 3.380281690140845,
            "step": 240
        },
        {
            "loss": 1.0276,
            "grad_norm": 7.978312015533447,
            "learning_rate": 3.6201669599689465e-05,
            "epoch": 3.52112676056338,
            "step": 250
        },
        {
            "loss": 1.01,
            "grad_norm": 10.280837059020996,
            "learning_rate": 3.520209030608662e-05,
            "epoch": 3.6619718309859155,
            "step": 260
        },
        {
            "loss": 0.9366,
            "grad_norm": 7.149789810180664,
            "learning_rate": 3.418253994161892e-05,
            "epoch": 3.802816901408451,
            "step": 270
        },
        {
            "loss": 1.0455,
            "grad_norm": 9.799720764160156,
            "learning_rate": 3.3145014324002944e-05,
            "epoch": 3.943661971830986,
            "step": 280
        },
        {
            "loss": 0.8563,
            "grad_norm": 13.855585098266602,
            "learning_rate": 3.209154445835742e-05,
            "epoch": 4.084507042253521,
            "step": 290
        },
        {
            "loss": 0.7457,
            "grad_norm": 8.40398120880127,
            "learning_rate": 3.102419256141536e-05,
            "epoch": 4.225352112676056,
            "step": 300
        },
        {
            "loss": 0.6687,
            "grad_norm": 11.543622016906738,
            "learning_rate": 2.9945048024637935e-05,
            "epoch": 4.366197183098592,
            "step": 310
        },
        {
            "loss": 0.6832,
            "grad_norm": 9.501200675964355,
            "learning_rate": 2.885622332413256e-05,
            "epoch": 4.507042253521127,
            "step": 320
        },
        {
            "loss": 0.653,
            "grad_norm": 7.726680278778076,
            "learning_rate": 2.775984988538175e-05,
            "epoch": 4.647887323943662,
            "step": 330
        },
        {
            "loss": 0.7454,
            "grad_norm": 8.7183198928833,
            "learning_rate": 2.6658073910877603e-05,
            "epoch": 4.788732394366197,
            "step": 340
        },
        {
            "loss": 0.6652,
            "grad_norm": 13.35969066619873,
            "learning_rate": 2.555305217882967e-05,
            "epoch": 4.929577464788732,
            "step": 350
        },
        {
            "loss": 0.5992,
            "grad_norm": 5.583257675170898,
            "learning_rate": 2.444694782117033e-05,
            "epoch": 5.070422535211268,
            "step": 360
        },
        {
            "loss": 0.4608,
            "grad_norm": 8.783230781555176,
            "learning_rate": 2.334192608912241e-05,
            "epoch": 5.211267605633803,
            "step": 370
        },
        {
            "loss": 0.4573,
            "grad_norm": 6.429036617279053,
            "learning_rate": 2.224015011461826e-05,
            "epoch": 5.352112676056338,
            "step": 380
        },
        {
            "loss": 0.4691,
            "grad_norm": 6.094832897186279,
            "learning_rate": 2.114377667586744e-05,
            "epoch": 5.492957746478873,
            "step": 390
        },
        {
            "loss": 0.4546,
            "grad_norm": 9.486061096191406,
            "learning_rate": 2.0054951975362067e-05,
            "epoch": 5.633802816901408,
            "step": 400
        },
        {
            "loss": 0.4643,
            "grad_norm": 7.3485541343688965,
            "learning_rate": 1.8975807438584642e-05,
            "epoch": 5.774647887323944,
            "step": 410
        },
        {
            "loss": 0.4924,
            "grad_norm": 13.411297798156738,
            "learning_rate": 1.7908455541642584e-05,
            "epoch": 5.915492957746479,
            "step": 420
        },
        {
            "loss": 0.4313,
            "grad_norm": 5.016476631164551,
            "learning_rate": 1.6854985675997066e-05,
            "epoch": 6.056338028169014,
            "step": 430
        },
        {
            "loss": 0.2946,
            "grad_norm": 6.291584491729736,
            "learning_rate": 1.5817460058381088e-05,
            "epoch": 6.197183098591549,
            "step": 440
        },
        {
            "loss": 0.416,
            "grad_norm": 7.728667259216309,
            "learning_rate": 1.4797909693913376e-05,
            "epoch": 6.338028169014084,
            "step": 450
        },
        {
            "loss": 0.3017,
            "grad_norm": 6.490198135375977,
            "learning_rate": 1.3798330400310539e-05,
            "epoch": 6.47887323943662,
            "step": 460
        },
        {
            "loss": 0.4019,
            "grad_norm": 5.814507007598877,
            "learning_rate": 1.2820678900980093e-05,
            "epoch": 6.619718309859155,
            "step": 470
        },
        {
            "loss": 0.3386,
            "grad_norm": 8.55227279663086,
            "learning_rate": 1.1866868994642535e-05,
            "epoch": 6.76056338028169,
            "step": 480
        },
        {
            "loss": 0.4076,
            "grad_norm": 5.631807327270508,
            "learning_rate": 1.0938767808980486e-05,
            "epoch": 6.901408450704225,
            "step": 490
        },
        {
            "loss": 0.3558,
            "grad_norm": 4.029697418212891,
            "learning_rate": 1.0038192145648567e-05,
            "epoch": 7.042253521126761,
            "step": 500
        },
        {
            "loss": 0.2694,
            "grad_norm": 4.9432454109191895,
            "learning_rate": 9.166904923798821e-06,
            "epoch": 7.183098591549296,
            "step": 510
        },
        {
            "loss": 0.331,
            "grad_norm": 3.9665186405181885,
            "learning_rate": 8.32661172908373e-06,
            "epoch": 7.323943661971831,
            "step": 520
        },
        {
            "loss": 0.2794,
            "grad_norm": 5.870142936706543,
            "learning_rate": 7.518957474892149e-06,
            "epoch": 7.464788732394366,
            "step": 530
        },
        {
            "loss": 0.2972,
            "grad_norm": 5.607622146606445,
            "learning_rate": 6.745523182354147e-06,
            "epoch": 7.605633802816901,
            "step": 540
        },
        {
            "loss": 0.2953,
            "grad_norm": 5.455047130584717,
            "learning_rate": 6.007822885417882e-06,
            "epoch": 7.746478873239437,
            "step": 550
        },
        {
            "loss": 0.3012,
            "grad_norm": 5.099092483520508,
            "learning_rate": 5.307300667057049e-06,
            "epoch": 7.887323943661972,
            "step": 560
        },
        {
            "loss": 0.2655,
            "grad_norm": 5.221102237701416,
            "learning_rate": 4.645327832410648e-06,
            "epoch": 8.028169014084508,
            "step": 570
        },
        {
            "loss": 0.2414,
            "grad_norm": 4.699168682098389,
            "learning_rate": 4.023200224388787e-06,
            "epoch": 8.169014084507042,
            "step": 580
        },
        {
            "loss": 0.232,
            "grad_norm": 3.3974449634552,
            "learning_rate": 3.4421356869993037e-06,
            "epoch": 8.309859154929578,
            "step": 590
        },
        {
            "loss": 0.261,
            "grad_norm": 6.071916103363037,
            "learning_rate": 2.9032716813609723e-06,
            "epoch": 8.450704225352112,
            "step": 600
        },
        {
            "loss": 0.2892,
            "grad_norm": 4.334784030914307,
            "learning_rate": 2.4076630590699062e-06,
            "epoch": 8.591549295774648,
            "step": 610
        },
        {
            "loss": 0.2661,
            "grad_norm": 7.854306221008301,
            "learning_rate": 1.956279997278043e-06,
            "epoch": 8.732394366197184,
            "step": 620
        },
        {
            "loss": 0.3008,
            "grad_norm": 5.541818618774414,
            "learning_rate": 1.5500060995258137e-06,
            "epoch": 8.873239436619718,
            "step": 630
        },
        {
            "loss": 0.2472,
            "grad_norm": 4.0823540687561035,
            "learning_rate": 1.1896366660467173e-06,
            "epoch": 9.014084507042254,
            "step": 640
        },
        {
            "loss": 0.2552,
            "grad_norm": 3.067195415496826,
            "learning_rate": 8.758771369297536e-07,
            "epoch": 9.154929577464788,
            "step": 650
        },
        {
            "loss": 0.263,
            "grad_norm": 4.276187896728516,
            "learning_rate": 6.093417111873306e-07,
            "epoch": 9.295774647887324,
            "step": 660
        },
        {
            "loss": 0.2458,
            "grad_norm": 6.139030933380127,
            "learning_rate": 3.905521444318605e-07,
            "epoch": 9.43661971830986,
            "step": 670
        },
        {
            "loss": 0.2303,
            "grad_norm": 2.5915589332580566,
            "learning_rate": 2.1993672751463579e-07,
            "epoch": 9.577464788732394,
            "step": 680
        },
        {
            "loss": 0.2806,
            "grad_norm": 4.341061592102051,
            "learning_rate": 9.782944812637973e-08,
            "epoch": 9.71830985915493,
            "step": 690
        },
        {
            "loss": 0.2293,
            "grad_norm": 3.5639548301696777,
            "learning_rate": 2.44693370006599e-08,
            "epoch": 9.859154929577464,
            "step": 700
        },
        {
            "loss": 0.2493,
            "grad_norm": 2.9560420513153076,
            "learning_rate": 0.0,
            "epoch": 10.0,
            "step": 710
        },
        {
            "train_runtime": 6201.3261,
            "train_samples_per_second": 1.83,
            "train_steps_per_second": 0.114,
            "total_flos": 2.624438456549376e+17,
            "train_loss": 1.053482675552368,
            "epoch": 10.0,
            "step": 710
        },
        {
            "eval_loss": 2.5100903511047363,
            "eval_runtime": 52.2887,
            "eval_samples_per_second": 5.431,
            "eval_steps_per_second": 0.688,
            "epoch": 10.0,
            "step": 710
        }
    ],
    "perplexity": 12.30604187476115
}