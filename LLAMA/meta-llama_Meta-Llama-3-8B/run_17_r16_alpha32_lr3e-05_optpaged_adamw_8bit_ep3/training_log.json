{
    "run_name": "run_17_r16_alpha32_lr3e-05_optpaged_adamw_8bit_ep3",
    "r": 16,
    "lora_alpha": 32,
    "optim": "paged_adamw_8bit",
    "learning_rate": 3e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.01,
    "num_train_epochs": 3,
    "train_log": [
        {
            "loss": 3.4553,
            "grad_norm": 1.725072979927063,
            "learning_rate": 2.859154929577465e-05,
            "epoch": 0.14084507042253522,
            "step": 10
        },
        {
            "loss": 3.3741,
            "grad_norm": 1.5281379222869873,
            "learning_rate": 2.7183098591549296e-05,
            "epoch": 0.28169014084507044,
            "step": 20
        },
        {
            "loss": 3.4574,
            "grad_norm": 2.1846044063568115,
            "learning_rate": 2.5774647887323944e-05,
            "epoch": 0.4225352112676056,
            "step": 30
        },
        {
            "loss": 2.9405,
            "grad_norm": 2.0285966396331787,
            "learning_rate": 2.436619718309859e-05,
            "epoch": 0.5633802816901409,
            "step": 40
        },
        {
            "loss": 2.9291,
            "grad_norm": 2.943582057952881,
            "learning_rate": 2.2957746478873243e-05,
            "epoch": 0.704225352112676,
            "step": 50
        },
        {
            "loss": 2.9857,
            "grad_norm": 2.8172457218170166,
            "learning_rate": 2.154929577464789e-05,
            "epoch": 0.8450704225352113,
            "step": 60
        },
        {
            "loss": 3.1319,
            "grad_norm": 3.229583740234375,
            "learning_rate": 2.0140845070422538e-05,
            "epoch": 0.9859154929577465,
            "step": 70
        },
        {
            "loss": 3.0325,
            "grad_norm": 3.688901662826538,
            "learning_rate": 1.8732394366197186e-05,
            "epoch": 1.1267605633802817,
            "step": 80
        },
        {
            "loss": 2.6754,
            "grad_norm": 3.8637912273406982,
            "learning_rate": 1.7323943661971833e-05,
            "epoch": 1.267605633802817,
            "step": 90
        },
        {
            "loss": 2.6408,
            "grad_norm": 4.361405372619629,
            "learning_rate": 1.5915492957746478e-05,
            "epoch": 1.408450704225352,
            "step": 100
        },
        {
            "loss": 2.6145,
            "grad_norm": 4.01881217956543,
            "learning_rate": 1.4507042253521127e-05,
            "epoch": 1.5492957746478875,
            "step": 110
        },
        {
            "loss": 2.5857,
            "grad_norm": 5.102541923522949,
            "learning_rate": 1.3098591549295775e-05,
            "epoch": 1.6901408450704225,
            "step": 120
        },
        {
            "loss": 2.321,
            "grad_norm": 4.414668083190918,
            "learning_rate": 1.1690140845070424e-05,
            "epoch": 1.8309859154929577,
            "step": 130
        },
        {
            "loss": 2.4412,
            "grad_norm": 5.8149094581604,
            "learning_rate": 1.0281690140845072e-05,
            "epoch": 1.971830985915493,
            "step": 140
        },
        {
            "loss": 2.5541,
            "grad_norm": 5.528045654296875,
            "learning_rate": 8.873239436619718e-06,
            "epoch": 2.112676056338028,
            "step": 150
        },
        {
            "loss": 2.3451,
            "grad_norm": 6.483697414398193,
            "learning_rate": 7.464788732394367e-06,
            "epoch": 2.2535211267605635,
            "step": 160
        },
        {
            "loss": 2.2676,
            "grad_norm": 6.490740776062012,
            "learning_rate": 6.056338028169014e-06,
            "epoch": 2.3943661971830985,
            "step": 170
        },
        {
            "loss": 2.1243,
            "grad_norm": 5.691077709197998,
            "learning_rate": 4.6478873239436615e-06,
            "epoch": 2.535211267605634,
            "step": 180
        },
        {
            "loss": 2.2415,
            "grad_norm": 5.0189995765686035,
            "learning_rate": 3.23943661971831e-06,
            "epoch": 2.676056338028169,
            "step": 190
        },
        {
            "loss": 2.2431,
            "grad_norm": 5.832099914550781,
            "learning_rate": 1.8309859154929577e-06,
            "epoch": 2.816901408450704,
            "step": 200
        },
        {
            "loss": 2.2149,
            "grad_norm": 5.209300994873047,
            "learning_rate": 4.225352112676057e-07,
            "epoch": 2.9577464788732395,
            "step": 210
        },
        {
            "train_runtime": 1860.4411,
            "train_samples_per_second": 1.83,
            "train_steps_per_second": 0.114,
            "total_flos": 7.896359453589504e+16,
            "train_loss": 2.690679756128732,
            "epoch": 3.0,
            "step": 213
        },
        {
            "eval_loss": 2.5975565910339355,
            "eval_runtime": 52.2582,
            "eval_samples_per_second": 5.435,
            "eval_steps_per_second": 0.689,
            "epoch": 3.0,
            "step": 213
        }
    ],
    "perplexity": 13.430880774979256
}