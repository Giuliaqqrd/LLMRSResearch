{
    "run_name": "run_7_r8_alpha16_lr3e-05_optpaged_adamw_8bit_ep10",
    "r": 8,
    "lora_alpha": 16,
    "optim": "paged_adamw_8bit",
    "learning_rate": 3e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.01,
    "num_train_epochs": 10,
    "train_log": [
        {
            "loss": 3.4744,
            "grad_norm": 1.285548210144043,
            "learning_rate": 2.9577464788732395e-05,
            "epoch": 0.14084507042253522,
            "step": 10
        },
        {
            "loss": 3.4365,
            "grad_norm": 1.257650375366211,
            "learning_rate": 2.915492957746479e-05,
            "epoch": 0.28169014084507044,
            "step": 20
        },
        {
            "loss": 3.5698,
            "grad_norm": 1.7682194709777832,
            "learning_rate": 2.873239436619718e-05,
            "epoch": 0.4225352112676056,
            "step": 30
        },
        {
            "loss": 3.0505,
            "grad_norm": 1.6582032442092896,
            "learning_rate": 2.8309859154929576e-05,
            "epoch": 0.5633802816901409,
            "step": 40
        },
        {
            "loss": 3.0283,
            "grad_norm": 2.3731274604797363,
            "learning_rate": 2.788732394366197e-05,
            "epoch": 0.704225352112676,
            "step": 50
        },
        {
            "loss": 3.0969,
            "grad_norm": 2.277656316757202,
            "learning_rate": 2.746478873239437e-05,
            "epoch": 0.8450704225352113,
            "step": 60
        },
        {
            "loss": 3.2351,
            "grad_norm": 2.870291233062744,
            "learning_rate": 2.7042253521126763e-05,
            "epoch": 0.9859154929577465,
            "step": 70
        },
        {
            "loss": 3.1623,
            "grad_norm": 3.246110677719116,
            "learning_rate": 2.6619718309859158e-05,
            "epoch": 1.1267605633802817,
            "step": 80
        },
        {
            "loss": 2.8057,
            "grad_norm": 3.5106098651885986,
            "learning_rate": 2.619718309859155e-05,
            "epoch": 1.267605633802817,
            "step": 90
        },
        {
            "loss": 2.7563,
            "grad_norm": 3.8836662769317627,
            "learning_rate": 2.5774647887323944e-05,
            "epoch": 1.408450704225352,
            "step": 100
        },
        {
            "loss": 2.7148,
            "grad_norm": 3.6609323024749756,
            "learning_rate": 2.535211267605634e-05,
            "epoch": 1.5492957746478875,
            "step": 110
        },
        {
            "loss": 2.6876,
            "grad_norm": 4.7757744789123535,
            "learning_rate": 2.4929577464788733e-05,
            "epoch": 1.6901408450704225,
            "step": 120
        },
        {
            "loss": 2.4036,
            "grad_norm": 3.8783769607543945,
            "learning_rate": 2.4507042253521128e-05,
            "epoch": 1.8309859154929577,
            "step": 130
        },
        {
            "loss": 2.5162,
            "grad_norm": 5.418384075164795,
            "learning_rate": 2.4084507042253522e-05,
            "epoch": 1.971830985915493,
            "step": 140
        },
        {
            "loss": 2.5937,
            "grad_norm": 5.470067501068115,
            "learning_rate": 2.3661971830985914e-05,
            "epoch": 2.112676056338028,
            "step": 150
        },
        {
            "loss": 2.3674,
            "grad_norm": 6.561141014099121,
            "learning_rate": 2.3239436619718308e-05,
            "epoch": 2.2535211267605635,
            "step": 160
        },
        {
            "loss": 2.266,
            "grad_norm": 6.7338128089904785,
            "learning_rate": 2.2816901408450703e-05,
            "epoch": 2.3943661971830985,
            "step": 170
        },
        {
            "loss": 2.1133,
            "grad_norm": 7.387811183929443,
            "learning_rate": 2.23943661971831e-05,
            "epoch": 2.535211267605634,
            "step": 180
        },
        {
            "loss": 2.1951,
            "grad_norm": 5.195838928222656,
            "learning_rate": 2.1971830985915496e-05,
            "epoch": 2.676056338028169,
            "step": 190
        },
        {
            "loss": 2.1683,
            "grad_norm": 6.783318519592285,
            "learning_rate": 2.154929577464789e-05,
            "epoch": 2.816901408450704,
            "step": 200
        },
        {
            "loss": 2.088,
            "grad_norm": 6.2389421463012695,
            "learning_rate": 2.112676056338028e-05,
            "epoch": 2.9577464788732395,
            "step": 210
        },
        {
            "loss": 2.1513,
            "grad_norm": 9.566668510437012,
            "learning_rate": 2.0704225352112676e-05,
            "epoch": 3.0985915492957745,
            "step": 220
        },
        {
            "loss": 1.8614,
            "grad_norm": 6.800897121429443,
            "learning_rate": 2.028169014084507e-05,
            "epoch": 3.23943661971831,
            "step": 230
        },
        {
            "loss": 1.8736,
            "grad_norm": 6.979598522186279,
            "learning_rate": 1.9859154929577465e-05,
            "epoch": 3.380281690140845,
            "step": 240
        },
        {
            "loss": 1.7169,
            "grad_norm": 8.065034866333008,
            "learning_rate": 1.943661971830986e-05,
            "epoch": 3.52112676056338,
            "step": 250
        },
        {
            "loss": 1.6996,
            "grad_norm": 9.345511436462402,
            "learning_rate": 1.9014084507042255e-05,
            "epoch": 3.6619718309859155,
            "step": 260
        },
        {
            "loss": 1.6106,
            "grad_norm": 7.429489612579346,
            "learning_rate": 1.8591549295774646e-05,
            "epoch": 3.802816901408451,
            "step": 270
        },
        {
            "loss": 1.7059,
            "grad_norm": 9.511666297912598,
            "learning_rate": 1.816901408450704e-05,
            "epoch": 3.943661971830986,
            "step": 280
        },
        {
            "loss": 1.6515,
            "grad_norm": 13.025652885437012,
            "learning_rate": 1.7746478873239435e-05,
            "epoch": 4.084507042253521,
            "step": 290
        },
        {
            "loss": 1.4549,
            "grad_norm": 7.82762336730957,
            "learning_rate": 1.7323943661971833e-05,
            "epoch": 4.225352112676056,
            "step": 300
        },
        {
            "loss": 1.3983,
            "grad_norm": 13.158321380615234,
            "learning_rate": 1.6901408450704228e-05,
            "epoch": 4.366197183098592,
            "step": 310
        },
        {
            "loss": 1.3113,
            "grad_norm": 11.30390453338623,
            "learning_rate": 1.6478873239436623e-05,
            "epoch": 4.507042253521127,
            "step": 320
        },
        {
            "loss": 1.2797,
            "grad_norm": 8.74326229095459,
            "learning_rate": 1.6056338028169014e-05,
            "epoch": 4.647887323943662,
            "step": 330
        },
        {
            "loss": 1.4522,
            "grad_norm": 9.04911994934082,
            "learning_rate": 1.563380281690141e-05,
            "epoch": 4.788732394366197,
            "step": 340
        },
        {
            "loss": 1.3305,
            "grad_norm": 13.003854751586914,
            "learning_rate": 1.5211267605633803e-05,
            "epoch": 4.929577464788732,
            "step": 350
        },
        {
            "loss": 1.2418,
            "grad_norm": 9.654558181762695,
            "learning_rate": 1.4788732394366198e-05,
            "epoch": 5.070422535211268,
            "step": 360
        },
        {
            "loss": 1.0421,
            "grad_norm": 9.608189582824707,
            "learning_rate": 1.436619718309859e-05,
            "epoch": 5.211267605633803,
            "step": 370
        },
        {
            "loss": 1.1129,
            "grad_norm": 9.156543731689453,
            "learning_rate": 1.3943661971830985e-05,
            "epoch": 5.352112676056338,
            "step": 380
        },
        {
            "loss": 1.0938,
            "grad_norm": 10.842815399169922,
            "learning_rate": 1.3521126760563382e-05,
            "epoch": 5.492957746478873,
            "step": 390
        },
        {
            "loss": 1.0394,
            "grad_norm": 15.331591606140137,
            "learning_rate": 1.3098591549295775e-05,
            "epoch": 5.633802816901408,
            "step": 400
        },
        {
            "loss": 0.9891,
            "grad_norm": 11.080533981323242,
            "learning_rate": 1.267605633802817e-05,
            "epoch": 5.774647887323944,
            "step": 410
        },
        {
            "loss": 1.0802,
            "grad_norm": 15.365032196044922,
            "learning_rate": 1.2253521126760564e-05,
            "epoch": 5.915492957746479,
            "step": 420
        },
        {
            "loss": 1.0402,
            "grad_norm": 9.11070442199707,
            "learning_rate": 1.1830985915492957e-05,
            "epoch": 6.056338028169014,
            "step": 430
        },
        {
            "loss": 0.8076,
            "grad_norm": 13.850679397583008,
            "learning_rate": 1.1408450704225351e-05,
            "epoch": 6.197183098591549,
            "step": 440
        },
        {
            "loss": 0.9548,
            "grad_norm": 9.992170333862305,
            "learning_rate": 1.0985915492957748e-05,
            "epoch": 6.338028169014084,
            "step": 450
        },
        {
            "loss": 0.7533,
            "grad_norm": 10.960161209106445,
            "learning_rate": 1.056338028169014e-05,
            "epoch": 6.47887323943662,
            "step": 460
        },
        {
            "loss": 0.813,
            "grad_norm": 7.987470626831055,
            "learning_rate": 1.0140845070422535e-05,
            "epoch": 6.619718309859155,
            "step": 470
        },
        {
            "loss": 0.8193,
            "grad_norm": 12.076507568359375,
            "learning_rate": 9.71830985915493e-06,
            "epoch": 6.76056338028169,
            "step": 480
        },
        {
            "loss": 0.8487,
            "grad_norm": 11.59890079498291,
            "learning_rate": 9.295774647887323e-06,
            "epoch": 6.901408450704225,
            "step": 490
        },
        {
            "loss": 0.8257,
            "grad_norm": 9.281187057495117,
            "learning_rate": 8.873239436619718e-06,
            "epoch": 7.042253521126761,
            "step": 500
        },
        {
            "loss": 0.6058,
            "grad_norm": 10.474023818969727,
            "learning_rate": 8.450704225352114e-06,
            "epoch": 7.183098591549296,
            "step": 510
        },
        {
            "loss": 0.6983,
            "grad_norm": 7.869467735290527,
            "learning_rate": 8.028169014084507e-06,
            "epoch": 7.323943661971831,
            "step": 520
        },
        {
            "loss": 0.641,
            "grad_norm": 13.665165901184082,
            "learning_rate": 7.6056338028169015e-06,
            "epoch": 7.464788732394366,
            "step": 530
        },
        {
            "loss": 0.6682,
            "grad_norm": 8.368562698364258,
            "learning_rate": 7.183098591549295e-06,
            "epoch": 7.605633802816901,
            "step": 540
        },
        {
            "loss": 0.6857,
            "grad_norm": 10.388644218444824,
            "learning_rate": 6.760563380281691e-06,
            "epoch": 7.746478873239437,
            "step": 550
        },
        {
            "loss": 0.6831,
            "grad_norm": 10.477754592895508,
            "learning_rate": 6.338028169014085e-06,
            "epoch": 7.887323943661972,
            "step": 560
        },
        {
            "loss": 0.5688,
            "grad_norm": 11.809771537780762,
            "learning_rate": 5.915492957746478e-06,
            "epoch": 8.028169014084508,
            "step": 570
        },
        {
            "loss": 0.5283,
            "grad_norm": 10.355786323547363,
            "learning_rate": 5.492957746478874e-06,
            "epoch": 8.169014084507042,
            "step": 580
        },
        {
            "loss": 0.4899,
            "grad_norm": 11.060554504394531,
            "learning_rate": 5.070422535211268e-06,
            "epoch": 8.309859154929578,
            "step": 590
        },
        {
            "loss": 0.6031,
            "grad_norm": 13.250222206115723,
            "learning_rate": 4.6478873239436615e-06,
            "epoch": 8.450704225352112,
            "step": 600
        },
        {
            "loss": 0.5849,
            "grad_norm": 11.677823066711426,
            "learning_rate": 4.225352112676057e-06,
            "epoch": 8.591549295774648,
            "step": 610
        },
        {
            "loss": 0.6481,
            "grad_norm": 12.538334846496582,
            "learning_rate": 3.8028169014084508e-06,
            "epoch": 8.732394366197184,
            "step": 620
        },
        {
            "loss": 0.6096,
            "grad_norm": 13.574053764343262,
            "learning_rate": 3.3802816901408454e-06,
            "epoch": 8.873239436619718,
            "step": 630
        },
        {
            "loss": 0.4728,
            "grad_norm": 7.924614906311035,
            "learning_rate": 2.957746478873239e-06,
            "epoch": 9.014084507042254,
            "step": 640
        },
        {
            "loss": 0.4769,
            "grad_norm": 7.301567077636719,
            "learning_rate": 2.535211267605634e-06,
            "epoch": 9.154929577464788,
            "step": 650
        },
        {
            "loss": 0.4816,
            "grad_norm": 9.178017616271973,
            "learning_rate": 2.1126760563380285e-06,
            "epoch": 9.295774647887324,
            "step": 660
        },
        {
            "loss": 0.5144,
            "grad_norm": 8.510727882385254,
            "learning_rate": 1.6901408450704227e-06,
            "epoch": 9.43661971830986,
            "step": 670
        },
        {
            "loss": 0.4485,
            "grad_norm": 7.204808712005615,
            "learning_rate": 1.267605633802817e-06,
            "epoch": 9.577464788732394,
            "step": 680
        },
        {
            "loss": 0.5556,
            "grad_norm": 10.871257781982422,
            "learning_rate": 8.450704225352114e-07,
            "epoch": 9.71830985915493,
            "step": 690
        },
        {
            "loss": 0.4958,
            "grad_norm": 10.414268493652344,
            "learning_rate": 4.225352112676057e-07,
            "epoch": 9.859154929577464,
            "step": 700
        },
        {
            "loss": 0.4616,
            "grad_norm": 7.028232574462891,
            "learning_rate": 0.0,
            "epoch": 10.0,
            "step": 710
        },
        {
            "train_runtime": 6205.3474,
            "train_samples_per_second": 1.829,
            "train_steps_per_second": 0.114,
            "total_flos": 2.624438456549376e+17,
            "train_loss": 1.501562991612394,
            "epoch": 10.0,
            "step": 710
        },
        {
            "eval_loss": 2.441568613052368,
            "eval_runtime": 52.3307,
            "eval_samples_per_second": 5.427,
            "eval_steps_per_second": 0.688,
            "epoch": 10.0,
            "step": 710
        }
    ],
    "perplexity": 11.491051626615004
}