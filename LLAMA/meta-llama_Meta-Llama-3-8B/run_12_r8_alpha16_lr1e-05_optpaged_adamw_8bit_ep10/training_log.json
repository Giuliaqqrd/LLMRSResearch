{
    "run_name": "run_12_r8_alpha16_lr1e-05_optpaged_adamw_8bit_ep10",
    "r": 8,
    "lora_alpha": 16,
    "optim": "paged_adamw_8bit",
    "learning_rate": 1e-05,
    "lr_scheduler": "cosine",
    "weight_decay": 0.01,
    "num_train_epochs": 10,
    "train_log": [
        {
            "loss": 3.4886,
            "grad_norm": 1.0772055387496948,
            "learning_rate": 9.995106132599869e-06,
            "epoch": 0.14084507042253522,
            "step": 10
        },
        {
            "loss": 3.4959,
            "grad_norm": 0.8646368980407715,
            "learning_rate": 9.980434110374725e-06,
            "epoch": 0.28169014084507044,
            "step": 20
        },
        {
            "loss": 3.7176,
            "grad_norm": 1.073046088218689,
            "learning_rate": 9.956012654497073e-06,
            "epoch": 0.4225352112676056,
            "step": 30
        },
        {
            "loss": 3.2729,
            "grad_norm": 1.221330165863037,
            "learning_rate": 9.921889571113629e-06,
            "epoch": 0.5633802816901409,
            "step": 40
        },
        {
            "loss": 3.3106,
            "grad_norm": 1.494736671447754,
            "learning_rate": 9.878131657762535e-06,
            "epoch": 0.704225352112676,
            "step": 50
        },
        {
            "loss": 3.4485,
            "grad_norm": 1.4406427145004272,
            "learning_rate": 9.82482457261405e-06,
            "epoch": 0.8450704225352113,
            "step": 60
        },
        {
            "loss": 3.6207,
            "grad_norm": 2.021904468536377,
            "learning_rate": 9.762072666790658e-06,
            "epoch": 0.9859154929577465,
            "step": 70
        },
        {
            "loss": 3.6348,
            "grad_norm": 2.1515917778015137,
            "learning_rate": 9.689998780094839e-06,
            "epoch": 1.1267605633802817,
            "step": 80
        },
        {
            "loss": 3.3279,
            "grad_norm": 2.3054540157318115,
            "learning_rate": 9.608744000544392e-06,
            "epoch": 1.267605633802817,
            "step": 90
        },
        {
            "loss": 3.2658,
            "grad_norm": 2.410839080810547,
            "learning_rate": 9.51846738818602e-06,
            "epoch": 1.408450704225352,
            "step": 100
        },
        {
            "loss": 3.1857,
            "grad_norm": 2.1888208389282227,
            "learning_rate": 9.419345663727805e-06,
            "epoch": 1.5492957746478875,
            "step": 110
        },
        {
            "loss": 3.2085,
            "grad_norm": 3.1362805366516113,
            "learning_rate": 9.31157286260014e-06,
            "epoch": 1.6901408450704225,
            "step": 120
        },
        {
            "loss": 2.8926,
            "grad_norm": 2.2071175575256348,
            "learning_rate": 9.195359955122244e-06,
            "epoch": 1.8309859154929577,
            "step": 130
        },
        {
            "loss": 3.0228,
            "grad_norm": 2.6504430770874023,
            "learning_rate": 9.070934433517872e-06,
            "epoch": 1.971830985915493,
            "step": 140
        },
        {
            "loss": 3.2021,
            "grad_norm": 2.655197858810425,
            "learning_rate": 8.938539866588593e-06,
            "epoch": 2.112676056338028,
            "step": 150
        },
        {
            "loss": 2.9821,
            "grad_norm": 3.155320405960083,
            "learning_rate": 8.798435422916425e-06,
            "epoch": 2.2535211267605635,
            "step": 160
        },
        {
            "loss": 3.0546,
            "grad_norm": 3.208577871322632,
            "learning_rate": 8.650895363529172e-06,
            "epoch": 2.3943661971830985,
            "step": 170
        },
        {
            "loss": 2.7881,
            "grad_norm": 2.8236629962921143,
            "learning_rate": 8.496208505021572e-06,
            "epoch": 2.535211267605634,
            "step": 180
        },
        {
            "loss": 2.884,
            "grad_norm": 2.5107314586639404,
            "learning_rate": 8.334677654183254e-06,
            "epoch": 2.676056338028169,
            "step": 190
        },
        {
            "loss": 2.8949,
            "grad_norm": 3.5583548545837402,
            "learning_rate": 8.166619015240236e-06,
            "epoch": 2.816901408450704,
            "step": 200
        },
        {
            "loss": 2.9492,
            "grad_norm": 2.8285090923309326,
            "learning_rate": 7.992361570870289e-06,
            "epoch": 2.9577464788732395,
            "step": 210
        },
        {
            "loss": 3.0212,
            "grad_norm": 4.27040433883667,
            "learning_rate": 7.812246438203905e-06,
            "epoch": 3.0985915492957745,
            "step": 220
        },
        {
            "loss": 2.7762,
            "grad_norm": 3.1389729976654053,
            "learning_rate": 7.626626201071494e-06,
            "epoch": 3.23943661971831,
            "step": 230
        },
        {
            "loss": 2.7704,
            "grad_norm": 2.8003365993499756,
            "learning_rate": 7.4358642198039835e-06,
            "epoch": 3.380281690140845,
            "step": 240
        },
        {
            "loss": 2.715,
            "grad_norm": 3.240147352218628,
            "learning_rate": 7.240333919937893e-06,
            "epoch": 3.52112676056338,
            "step": 250
        },
        {
            "loss": 2.717,
            "grad_norm": 3.806659698486328,
            "learning_rate": 7.040418061217325e-06,
            "epoch": 3.6619718309859155,
            "step": 260
        },
        {
            "loss": 2.6928,
            "grad_norm": 3.518564224243164,
            "learning_rate": 6.836507988323785e-06,
            "epoch": 3.802816901408451,
            "step": 270
        },
        {
            "loss": 2.6958,
            "grad_norm": 4.387148380279541,
            "learning_rate": 6.629002864800589e-06,
            "epoch": 3.943661971830986,
            "step": 280
        },
        {
            "loss": 2.792,
            "grad_norm": 5.626343250274658,
            "learning_rate": 6.418308891671484e-06,
            "epoch": 4.084507042253521,
            "step": 290
        },
        {
            "loss": 2.664,
            "grad_norm": 3.0746846199035645,
            "learning_rate": 6.204838512283073e-06,
            "epoch": 4.225352112676056,
            "step": 300
        },
        {
            "loss": 2.5854,
            "grad_norm": 4.358467102050781,
            "learning_rate": 5.989009604927587e-06,
            "epoch": 4.366197183098592,
            "step": 310
        },
        {
            "loss": 2.4873,
            "grad_norm": 5.0399322509765625,
            "learning_rate": 5.771244664826512e-06,
            "epoch": 4.507042253521127,
            "step": 320
        },
        {
            "loss": 2.4585,
            "grad_norm": 4.463275909423828,
            "learning_rate": 5.55196997707635e-06,
            "epoch": 4.647887323943662,
            "step": 330
        },
        {
            "loss": 2.694,
            "grad_norm": 3.790027141571045,
            "learning_rate": 5.33161478217552e-06,
            "epoch": 4.788732394366197,
            "step": 340
        },
        {
            "loss": 2.5343,
            "grad_norm": 4.872878074645996,
            "learning_rate": 5.110610435765935e-06,
            "epoch": 4.929577464788732,
            "step": 350
        },
        {
            "loss": 2.5978,
            "grad_norm": 3.838268518447876,
            "learning_rate": 4.8893895642340665e-06,
            "epoch": 5.070422535211268,
            "step": 360
        },
        {
            "loss": 2.4874,
            "grad_norm": 4.2769694328308105,
            "learning_rate": 4.668385217824482e-06,
            "epoch": 5.211267605633803,
            "step": 370
        },
        {
            "loss": 2.5375,
            "grad_norm": 3.962425470352173,
            "learning_rate": 4.4480300229236525e-06,
            "epoch": 5.352112676056338,
            "step": 380
        },
        {
            "loss": 2.4694,
            "grad_norm": 4.200253486633301,
            "learning_rate": 4.228755335173488e-06,
            "epoch": 5.492957746478873,
            "step": 390
        },
        {
            "loss": 2.4617,
            "grad_norm": 6.302429676055908,
            "learning_rate": 4.010990395072414e-06,
            "epoch": 5.633802816901408,
            "step": 400
        },
        {
            "loss": 2.2822,
            "grad_norm": 4.3512864112854,
            "learning_rate": 3.7951614877169285e-06,
            "epoch": 5.774647887323944,
            "step": 410
        },
        {
            "loss": 2.4964,
            "grad_norm": 6.781513690948486,
            "learning_rate": 3.5816911083285165e-06,
            "epoch": 5.915492957746479,
            "step": 420
        },
        {
            "loss": 2.5705,
            "grad_norm": 4.207509517669678,
            "learning_rate": 3.370997135199413e-06,
            "epoch": 6.056338028169014,
            "step": 430
        },
        {
            "loss": 2.3098,
            "grad_norm": 4.602911472320557,
            "learning_rate": 3.1634920116762175e-06,
            "epoch": 6.197183098591549,
            "step": 440
        },
        {
            "loss": 2.5548,
            "grad_norm": 4.602768421173096,
            "learning_rate": 2.9595819387826753e-06,
            "epoch": 6.338028169014084,
            "step": 450
        },
        {
            "loss": 2.1975,
            "grad_norm": 5.073617935180664,
            "learning_rate": 2.7596660800621076e-06,
            "epoch": 6.47887323943662,
            "step": 460
        },
        {
            "loss": 2.347,
            "grad_norm": 4.413206100463867,
            "learning_rate": 2.5641357801960186e-06,
            "epoch": 6.619718309859155,
            "step": 470
        },
        {
            "loss": 2.3822,
            "grad_norm": 6.356215476989746,
            "learning_rate": 2.373373798928507e-06,
            "epoch": 6.76056338028169,
            "step": 480
        },
        {
            "loss": 2.3054,
            "grad_norm": 6.932323932647705,
            "learning_rate": 2.187753561796097e-06,
            "epoch": 6.901408450704225,
            "step": 490
        },
        {
            "loss": 2.3729,
            "grad_norm": 5.220999240875244,
            "learning_rate": 2.0076384291297134e-06,
            "epoch": 7.042253521126761,
            "step": 500
        },
        {
            "loss": 2.2582,
            "grad_norm": 4.810103893280029,
            "learning_rate": 1.8333809847597644e-06,
            "epoch": 7.183098591549296,
            "step": 510
        },
        {
            "loss": 2.3128,
            "grad_norm": 4.347006320953369,
            "learning_rate": 1.665322345816746e-06,
            "epoch": 7.323943661971831,
            "step": 520
        },
        {
            "loss": 2.3357,
            "grad_norm": 6.999446392059326,
            "learning_rate": 1.50379149497843e-06,
            "epoch": 7.464788732394366,
            "step": 530
        },
        {
            "loss": 2.2941,
            "grad_norm": 4.229657173156738,
            "learning_rate": 1.3491046364708294e-06,
            "epoch": 7.605633802816901,
            "step": 540
        },
        {
            "loss": 2.3174,
            "grad_norm": 5.391266822814941,
            "learning_rate": 1.2015645770835765e-06,
            "epoch": 7.746478873239437,
            "step": 550
        },
        {
            "loss": 2.3645,
            "grad_norm": 5.361518383026123,
            "learning_rate": 1.0614601334114099e-06,
            "epoch": 7.887323943661972,
            "step": 560
        },
        {
            "loss": 2.1617,
            "grad_norm": 7.9748005867004395,
            "learning_rate": 9.290655664821296e-07,
            "epoch": 8.028169014084508,
            "step": 570
        },
        {
            "loss": 2.17,
            "grad_norm": 6.2347211837768555,
            "learning_rate": 8.046400448777575e-07,
            "epoch": 8.169014084507042,
            "step": 580
        },
        {
            "loss": 2.2216,
            "grad_norm": 4.631930828094482,
            "learning_rate": 6.884271373998608e-07,
            "epoch": 8.309859154929578,
            "step": 590
        },
        {
            "loss": 2.371,
            "grad_norm": 6.409324645996094,
            "learning_rate": 5.806543362721945e-07,
            "epoch": 8.450704225352112,
            "step": 600
        },
        {
            "loss": 2.3849,
            "grad_norm": 4.809666156768799,
            "learning_rate": 4.815326118139813e-07,
            "epoch": 8.591549295774648,
            "step": 610
        },
        {
            "loss": 2.3508,
            "grad_norm": 5.520517826080322,
            "learning_rate": 3.9125599945560866e-07,
            "epoch": 8.732394366197184,
            "step": 620
        },
        {
            "loss": 2.2748,
            "grad_norm": 5.92645788192749,
            "learning_rate": 3.100012199051627e-07,
            "epoch": 8.873239436619718,
            "step": 630
        },
        {
            "loss": 2.2358,
            "grad_norm": 5.260148525238037,
            "learning_rate": 2.3792733320934348e-07,
            "epoch": 9.014084507042254,
            "step": 640
        },
        {
            "loss": 2.3603,
            "grad_norm": 4.303272247314453,
            "learning_rate": 1.7517542738595071e-07,
            "epoch": 9.154929577464788,
            "step": 650
        },
        {
            "loss": 2.142,
            "grad_norm": 5.7818427085876465,
            "learning_rate": 1.2186834223746612e-07,
            "epoch": 9.295774647887324,
            "step": 660
        },
        {
            "loss": 2.3213,
            "grad_norm": 4.808958530426025,
            "learning_rate": 7.81104288863721e-08,
            "epoch": 9.43661971830986,
            "step": 670
        },
        {
            "loss": 2.1498,
            "grad_norm": 4.147306442260742,
            "learning_rate": 4.398734550292716e-08,
            "epoch": 9.577464788732394,
            "step": 680
        },
        {
            "loss": 2.4146,
            "grad_norm": 5.628302097320557,
            "learning_rate": 1.9565889625275945e-08,
            "epoch": 9.71830985915493,
            "step": 690
        },
        {
            "loss": 2.1893,
            "grad_norm": 5.225785732269287,
            "learning_rate": 4.89386740013198e-09,
            "epoch": 9.859154929577464,
            "step": 700
        },
        {
            "loss": 2.239,
            "grad_norm": 4.812194347381592,
            "learning_rate": 0.0,
            "epoch": 10.0,
            "step": 710
        },
        {
            "train_runtime": 6201.2481,
            "train_samples_per_second": 1.83,
            "train_steps_per_second": 0.114,
            "total_flos": 2.624438456549376e+17,
            "train_loss": 2.6689265694416746,
            "epoch": 10.0,
            "step": 710
        },
        {
            "eval_loss": 2.663181781768799,
            "eval_runtime": 52.2899,
            "eval_samples_per_second": 5.431,
            "eval_steps_per_second": 0.688,
            "epoch": 10.0,
            "step": 710
        }
    ],
    "perplexity": 14.341849213427961
}