{
    "run_name": "run_19_r16_alpha32_lr3e-05_optpaged_adamw_8bit_ep10",
    "r": 16,
    "lora_alpha": 32,
    "optim": "paged_adamw_8bit",
    "learning_rate": 3e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.01,
    "num_train_epochs": 10,
    "train_log": [
        {
            "loss": 3.4549,
            "grad_norm": 1.6673799753189087,
            "learning_rate": 2.9577464788732395e-05,
            "epoch": 0.14084507042253522,
            "step": 10
        },
        {
            "loss": 3.3717,
            "grad_norm": 1.483884572982788,
            "learning_rate": 2.915492957746479e-05,
            "epoch": 0.28169014084507044,
            "step": 20
        },
        {
            "loss": 3.447,
            "grad_norm": 2.1460001468658447,
            "learning_rate": 2.873239436619718e-05,
            "epoch": 0.4225352112676056,
            "step": 30
        },
        {
            "loss": 2.9217,
            "grad_norm": 1.98991858959198,
            "learning_rate": 2.8309859154929576e-05,
            "epoch": 0.5633802816901409,
            "step": 40
        },
        {
            "loss": 2.9064,
            "grad_norm": 2.9248206615448,
            "learning_rate": 2.788732394366197e-05,
            "epoch": 0.704225352112676,
            "step": 50
        },
        {
            "loss": 2.9504,
            "grad_norm": 2.8596420288085938,
            "learning_rate": 2.746478873239437e-05,
            "epoch": 0.8450704225352113,
            "step": 60
        },
        {
            "loss": 3.0849,
            "grad_norm": 3.2785751819610596,
            "learning_rate": 2.7042253521126763e-05,
            "epoch": 0.9859154929577465,
            "step": 70
        },
        {
            "loss": 2.9609,
            "grad_norm": 3.7871313095092773,
            "learning_rate": 2.6619718309859158e-05,
            "epoch": 1.1267605633802817,
            "step": 80
        },
        {
            "loss": 2.5782,
            "grad_norm": 4.318552017211914,
            "learning_rate": 2.619718309859155e-05,
            "epoch": 1.267605633802817,
            "step": 90
        },
        {
            "loss": 2.5307,
            "grad_norm": 4.818896770477295,
            "learning_rate": 2.5774647887323944e-05,
            "epoch": 1.408450704225352,
            "step": 100
        },
        {
            "loss": 2.5037,
            "grad_norm": 4.573021411895752,
            "learning_rate": 2.535211267605634e-05,
            "epoch": 1.5492957746478875,
            "step": 110
        },
        {
            "loss": 2.4431,
            "grad_norm": 6.1222405433654785,
            "learning_rate": 2.4929577464788733e-05,
            "epoch": 1.6901408450704225,
            "step": 120
        },
        {
            "loss": 2.1681,
            "grad_norm": 5.237565517425537,
            "learning_rate": 2.4507042253521128e-05,
            "epoch": 1.8309859154929577,
            "step": 130
        },
        {
            "loss": 2.2827,
            "grad_norm": 7.155482769012451,
            "learning_rate": 2.4084507042253522e-05,
            "epoch": 1.971830985915493,
            "step": 140
        },
        {
            "loss": 2.2513,
            "grad_norm": 7.001121997833252,
            "learning_rate": 2.3661971830985914e-05,
            "epoch": 2.112676056338028,
            "step": 150
        },
        {
            "loss": 2.0172,
            "grad_norm": 8.044981956481934,
            "learning_rate": 2.3239436619718308e-05,
            "epoch": 2.2535211267605635,
            "step": 160
        },
        {
            "loss": 1.8936,
            "grad_norm": 8.505897521972656,
            "learning_rate": 2.2816901408450703e-05,
            "epoch": 2.3943661971830985,
            "step": 170
        },
        {
            "loss": 1.7332,
            "grad_norm": 8.275236129760742,
            "learning_rate": 2.23943661971831e-05,
            "epoch": 2.535211267605634,
            "step": 180
        },
        {
            "loss": 1.8234,
            "grad_norm": 6.6498799324035645,
            "learning_rate": 2.1971830985915496e-05,
            "epoch": 2.676056338028169,
            "step": 190
        },
        {
            "loss": 1.7849,
            "grad_norm": 8.538025856018066,
            "learning_rate": 2.154929577464789e-05,
            "epoch": 2.816901408450704,
            "step": 200
        },
        {
            "loss": 1.6574,
            "grad_norm": 7.395996570587158,
            "learning_rate": 2.112676056338028e-05,
            "epoch": 2.9577464788732395,
            "step": 210
        },
        {
            "loss": 1.6747,
            "grad_norm": 12.533060073852539,
            "learning_rate": 2.0704225352112676e-05,
            "epoch": 3.0985915492957745,
            "step": 220
        },
        {
            "loss": 1.3895,
            "grad_norm": 7.859874725341797,
            "learning_rate": 2.028169014084507e-05,
            "epoch": 3.23943661971831,
            "step": 230
        },
        {
            "loss": 1.4111,
            "grad_norm": 8.5045747756958,
            "learning_rate": 1.9859154929577465e-05,
            "epoch": 3.380281690140845,
            "step": 240
        },
        {
            "loss": 1.2777,
            "grad_norm": 8.283519744873047,
            "learning_rate": 1.943661971830986e-05,
            "epoch": 3.52112676056338,
            "step": 250
        },
        {
            "loss": 1.2622,
            "grad_norm": 10.745613098144531,
            "learning_rate": 1.9014084507042255e-05,
            "epoch": 3.6619718309859155,
            "step": 260
        },
        {
            "loss": 1.1735,
            "grad_norm": 7.932107925415039,
            "learning_rate": 1.8591549295774646e-05,
            "epoch": 3.802816901408451,
            "step": 270
        },
        {
            "loss": 1.279,
            "grad_norm": 10.648598670959473,
            "learning_rate": 1.816901408450704e-05,
            "epoch": 3.943661971830986,
            "step": 280
        },
        {
            "loss": 1.1511,
            "grad_norm": 15.322366714477539,
            "learning_rate": 1.7746478873239435e-05,
            "epoch": 4.084507042253521,
            "step": 290
        },
        {
            "loss": 0.9966,
            "grad_norm": 9.398987770080566,
            "learning_rate": 1.7323943661971833e-05,
            "epoch": 4.225352112676056,
            "step": 300
        },
        {
            "loss": 0.9225,
            "grad_norm": 12.445595741271973,
            "learning_rate": 1.6901408450704228e-05,
            "epoch": 4.366197183098592,
            "step": 310
        },
        {
            "loss": 0.8977,
            "grad_norm": 11.2442045211792,
            "learning_rate": 1.6478873239436623e-05,
            "epoch": 4.507042253521127,
            "step": 320
        },
        {
            "loss": 0.8596,
            "grad_norm": 9.263463973999023,
            "learning_rate": 1.6056338028169014e-05,
            "epoch": 4.647887323943662,
            "step": 330
        },
        {
            "loss": 0.986,
            "grad_norm": 9.393068313598633,
            "learning_rate": 1.563380281690141e-05,
            "epoch": 4.788732394366197,
            "step": 340
        },
        {
            "loss": 0.8793,
            "grad_norm": 14.977513313293457,
            "learning_rate": 1.5211267605633803e-05,
            "epoch": 4.929577464788732,
            "step": 350
        },
        {
            "loss": 0.7957,
            "grad_norm": 7.595856666564941,
            "learning_rate": 1.4788732394366198e-05,
            "epoch": 5.070422535211268,
            "step": 360
        },
        {
            "loss": 0.6243,
            "grad_norm": 10.69156551361084,
            "learning_rate": 1.436619718309859e-05,
            "epoch": 5.211267605633803,
            "step": 370
        },
        {
            "loss": 0.6551,
            "grad_norm": 8.425065994262695,
            "learning_rate": 1.3943661971830985e-05,
            "epoch": 5.352112676056338,
            "step": 380
        },
        {
            "loss": 0.6342,
            "grad_norm": 9.084169387817383,
            "learning_rate": 1.3521126760563382e-05,
            "epoch": 5.492957746478873,
            "step": 390
        },
        {
            "loss": 0.6313,
            "grad_norm": 14.211969375610352,
            "learning_rate": 1.3098591549295775e-05,
            "epoch": 5.633802816901408,
            "step": 400
        },
        {
            "loss": 0.6311,
            "grad_norm": 10.422042846679688,
            "learning_rate": 1.267605633802817e-05,
            "epoch": 5.774647887323944,
            "step": 410
        },
        {
            "loss": 0.6603,
            "grad_norm": 14.38406753540039,
            "learning_rate": 1.2253521126760564e-05,
            "epoch": 5.915492957746479,
            "step": 420
        },
        {
            "loss": 0.6129,
            "grad_norm": 7.953084945678711,
            "learning_rate": 1.1830985915492957e-05,
            "epoch": 6.056338028169014,
            "step": 430
        },
        {
            "loss": 0.4145,
            "grad_norm": 10.631233215332031,
            "learning_rate": 1.1408450704225351e-05,
            "epoch": 6.197183098591549,
            "step": 440
        },
        {
            "loss": 0.5401,
            "grad_norm": 8.277295112609863,
            "learning_rate": 1.0985915492957748e-05,
            "epoch": 6.338028169014084,
            "step": 450
        },
        {
            "loss": 0.4113,
            "grad_norm": 10.157353401184082,
            "learning_rate": 1.056338028169014e-05,
            "epoch": 6.47887323943662,
            "step": 460
        },
        {
            "loss": 0.5122,
            "grad_norm": 7.591229438781738,
            "learning_rate": 1.0140845070422535e-05,
            "epoch": 6.619718309859155,
            "step": 470
        },
        {
            "loss": 0.4626,
            "grad_norm": 10.862618446350098,
            "learning_rate": 9.71830985915493e-06,
            "epoch": 6.76056338028169,
            "step": 480
        },
        {
            "loss": 0.5188,
            "grad_norm": 10.236244201660156,
            "learning_rate": 9.295774647887323e-06,
            "epoch": 6.901408450704225,
            "step": 490
        },
        {
            "loss": 0.4597,
            "grad_norm": 7.544020175933838,
            "learning_rate": 8.873239436619718e-06,
            "epoch": 7.042253521126761,
            "step": 500
        },
        {
            "loss": 0.334,
            "grad_norm": 8.321796417236328,
            "learning_rate": 8.450704225352114e-06,
            "epoch": 7.183098591549296,
            "step": 510
        },
        {
            "loss": 0.4332,
            "grad_norm": 5.2269816398620605,
            "learning_rate": 8.028169014084507e-06,
            "epoch": 7.323943661971831,
            "step": 520
        },
        {
            "loss": 0.3379,
            "grad_norm": 7.82636022567749,
            "learning_rate": 7.6056338028169015e-06,
            "epoch": 7.464788732394366,
            "step": 530
        },
        {
            "loss": 0.3743,
            "grad_norm": 7.5153303146362305,
            "learning_rate": 7.183098591549295e-06,
            "epoch": 7.605633802816901,
            "step": 540
        },
        {
            "loss": 0.3747,
            "grad_norm": 8.010839462280273,
            "learning_rate": 6.760563380281691e-06,
            "epoch": 7.746478873239437,
            "step": 550
        },
        {
            "loss": 0.3824,
            "grad_norm": 6.160958290100098,
            "learning_rate": 6.338028169014085e-06,
            "epoch": 7.887323943661972,
            "step": 560
        },
        {
            "loss": 0.3248,
            "grad_norm": 5.961624622344971,
            "learning_rate": 5.915492957746478e-06,
            "epoch": 8.028169014084508,
            "step": 570
        },
        {
            "loss": 0.2887,
            "grad_norm": 7.019592761993408,
            "learning_rate": 5.492957746478874e-06,
            "epoch": 8.169014084507042,
            "step": 580
        },
        {
            "loss": 0.2714,
            "grad_norm": 6.25002908706665,
            "learning_rate": 5.070422535211268e-06,
            "epoch": 8.309859154929578,
            "step": 590
        },
        {
            "loss": 0.3181,
            "grad_norm": 8.060894012451172,
            "learning_rate": 4.6478873239436615e-06,
            "epoch": 8.450704225352112,
            "step": 600
        },
        {
            "loss": 0.3465,
            "grad_norm": 4.993382930755615,
            "learning_rate": 4.225352112676057e-06,
            "epoch": 8.591549295774648,
            "step": 610
        },
        {
            "loss": 0.3465,
            "grad_norm": 12.792542457580566,
            "learning_rate": 3.8028169014084508e-06,
            "epoch": 8.732394366197184,
            "step": 620
        },
        {
            "loss": 0.3652,
            "grad_norm": 7.401532173156738,
            "learning_rate": 3.3802816901408454e-06,
            "epoch": 8.873239436619718,
            "step": 630
        },
        {
            "loss": 0.2878,
            "grad_norm": 4.416314601898193,
            "learning_rate": 2.957746478873239e-06,
            "epoch": 9.014084507042254,
            "step": 640
        },
        {
            "loss": 0.2776,
            "grad_norm": 3.7799859046936035,
            "learning_rate": 2.535211267605634e-06,
            "epoch": 9.154929577464788,
            "step": 650
        },
        {
            "loss": 0.2965,
            "grad_norm": 5.073003768920898,
            "learning_rate": 2.1126760563380285e-06,
            "epoch": 9.295774647887324,
            "step": 660
        },
        {
            "loss": 0.2904,
            "grad_norm": 9.032156944274902,
            "learning_rate": 1.6901408450704227e-06,
            "epoch": 9.43661971830986,
            "step": 670
        },
        {
            "loss": 0.2595,
            "grad_norm": 3.3018105030059814,
            "learning_rate": 1.267605633802817e-06,
            "epoch": 9.577464788732394,
            "step": 680
        },
        {
            "loss": 0.3107,
            "grad_norm": 5.349246025085449,
            "learning_rate": 8.450704225352114e-07,
            "epoch": 9.71830985915493,
            "step": 690
        },
        {
            "loss": 0.2701,
            "grad_norm": 5.321315765380859,
            "learning_rate": 4.225352112676057e-07,
            "epoch": 9.859154929577464,
            "step": 700
        },
        {
            "loss": 0.281,
            "grad_norm": 3.900679111480713,
            "learning_rate": 0.0,
            "epoch": 10.0,
            "step": 710
        },
        {
            "train_runtime": 6199.217,
            "train_samples_per_second": 1.831,
            "train_steps_per_second": 0.115,
            "total_flos": 2.632119817863168e+17,
            "train_loss": 1.1867795561400938,
            "epoch": 10.0,
            "step": 710
        },
        {
            "eval_loss": 2.558864116668701,
            "eval_runtime": 52.2468,
            "eval_samples_per_second": 5.436,
            "eval_steps_per_second": 0.689,
            "epoch": 10.0,
            "step": 710
        }
    ],
    "perplexity": 12.921132078214718
}