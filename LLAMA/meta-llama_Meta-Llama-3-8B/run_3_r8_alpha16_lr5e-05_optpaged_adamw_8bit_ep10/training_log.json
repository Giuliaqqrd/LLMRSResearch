{
    "run_name": "run_3_r8_alpha16_lr5e-05_optpaged_adamw_8bit_ep10",
    "r": 8,
    "lora_alpha": 16,
    "optim": "paged_adamw_8bit",
    "learning_rate": 5e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.01,
    "num_train_epochs": 10,
    "train_log": [
        {
            "loss": 3.4564,
            "grad_norm": 1.3873411417007446,
            "learning_rate": 4.929577464788733e-05,
            "epoch": 0.14084507042253522,
            "step": 10
        },
        {
            "loss": 3.3607,
            "grad_norm": 1.395033597946167,
            "learning_rate": 4.8591549295774653e-05,
            "epoch": 0.28169014084507044,
            "step": 20
        },
        {
            "loss": 3.4114,
            "grad_norm": 1.9965451955795288,
            "learning_rate": 4.788732394366197e-05,
            "epoch": 0.4225352112676056,
            "step": 30
        },
        {
            "loss": 2.8867,
            "grad_norm": 1.8586293458938599,
            "learning_rate": 4.71830985915493e-05,
            "epoch": 0.5633802816901409,
            "step": 40
        },
        {
            "loss": 2.8649,
            "grad_norm": 2.885772228240967,
            "learning_rate": 4.647887323943662e-05,
            "epoch": 0.704225352112676,
            "step": 50
        },
        {
            "loss": 2.9069,
            "grad_norm": 2.9831411838531494,
            "learning_rate": 4.577464788732395e-05,
            "epoch": 0.8450704225352113,
            "step": 60
        },
        {
            "loss": 3.0339,
            "grad_norm": 3.2476210594177246,
            "learning_rate": 4.507042253521127e-05,
            "epoch": 0.9859154929577465,
            "step": 70
        },
        {
            "loss": 2.8975,
            "grad_norm": 3.888015031814575,
            "learning_rate": 4.436619718309859e-05,
            "epoch": 1.1267605633802817,
            "step": 80
        },
        {
            "loss": 2.5024,
            "grad_norm": 4.511209011077881,
            "learning_rate": 4.366197183098591e-05,
            "epoch": 1.267605633802817,
            "step": 90
        },
        {
            "loss": 2.4488,
            "grad_norm": 5.026681423187256,
            "learning_rate": 4.295774647887324e-05,
            "epoch": 1.408450704225352,
            "step": 100
        },
        {
            "loss": 2.4254,
            "grad_norm": 4.8102707862854,
            "learning_rate": 4.225352112676056e-05,
            "epoch": 1.5492957746478875,
            "step": 110
        },
        {
            "loss": 2.3573,
            "grad_norm": 6.27697229385376,
            "learning_rate": 4.154929577464789e-05,
            "epoch": 1.6901408450704225,
            "step": 120
        },
        {
            "loss": 2.0791,
            "grad_norm": 5.591400623321533,
            "learning_rate": 4.0845070422535214e-05,
            "epoch": 1.8309859154929577,
            "step": 130
        },
        {
            "loss": 2.2012,
            "grad_norm": 7.294405937194824,
            "learning_rate": 4.014084507042254e-05,
            "epoch": 1.971830985915493,
            "step": 140
        },
        {
            "loss": 2.1036,
            "grad_norm": 7.400113582611084,
            "learning_rate": 3.943661971830986e-05,
            "epoch": 2.112676056338028,
            "step": 150
        },
        {
            "loss": 1.8735,
            "grad_norm": 8.425555229187012,
            "learning_rate": 3.8732394366197184e-05,
            "epoch": 2.2535211267605635,
            "step": 160
        },
        {
            "loss": 1.7489,
            "grad_norm": 9.085009574890137,
            "learning_rate": 3.802816901408451e-05,
            "epoch": 2.3943661971830985,
            "step": 170
        },
        {
            "loss": 1.5872,
            "grad_norm": 8.54813003540039,
            "learning_rate": 3.7323943661971835e-05,
            "epoch": 2.535211267605634,
            "step": 180
        },
        {
            "loss": 1.684,
            "grad_norm": 6.679196357727051,
            "learning_rate": 3.661971830985916e-05,
            "epoch": 2.676056338028169,
            "step": 190
        },
        {
            "loss": 1.6392,
            "grad_norm": 8.541547775268555,
            "learning_rate": 3.5915492957746486e-05,
            "epoch": 2.816901408450704,
            "step": 200
        },
        {
            "loss": 1.499,
            "grad_norm": 7.2935261726379395,
            "learning_rate": 3.5211267605633805e-05,
            "epoch": 2.9577464788732395,
            "step": 210
        },
        {
            "loss": 1.491,
            "grad_norm": 12.839313507080078,
            "learning_rate": 3.450704225352113e-05,
            "epoch": 3.0985915492957745,
            "step": 220
        },
        {
            "loss": 1.2084,
            "grad_norm": 7.7278923988342285,
            "learning_rate": 3.380281690140845e-05,
            "epoch": 3.23943661971831,
            "step": 230
        },
        {
            "loss": 1.2381,
            "grad_norm": 8.155241966247559,
            "learning_rate": 3.3098591549295775e-05,
            "epoch": 3.380281690140845,
            "step": 240
        },
        {
            "loss": 1.1137,
            "grad_norm": 7.99813985824585,
            "learning_rate": 3.23943661971831e-05,
            "epoch": 3.52112676056338,
            "step": 250
        },
        {
            "loss": 1.1059,
            "grad_norm": 10.112570762634277,
            "learning_rate": 3.1690140845070426e-05,
            "epoch": 3.6619718309859155,
            "step": 260
        },
        {
            "loss": 1.0281,
            "grad_norm": 7.436107635498047,
            "learning_rate": 3.0985915492957744e-05,
            "epoch": 3.802816901408451,
            "step": 270
        },
        {
            "loss": 1.1255,
            "grad_norm": 10.061883926391602,
            "learning_rate": 3.028169014084507e-05,
            "epoch": 3.943661971830986,
            "step": 280
        },
        {
            "loss": 0.9604,
            "grad_norm": 15.143685340881348,
            "learning_rate": 2.9577464788732395e-05,
            "epoch": 4.084507042253521,
            "step": 290
        },
        {
            "loss": 0.84,
            "grad_norm": 9.075568199157715,
            "learning_rate": 2.887323943661972e-05,
            "epoch": 4.225352112676056,
            "step": 300
        },
        {
            "loss": 0.7536,
            "grad_norm": 11.542214393615723,
            "learning_rate": 2.8169014084507046e-05,
            "epoch": 4.366197183098592,
            "step": 310
        },
        {
            "loss": 0.754,
            "grad_norm": 10.341604232788086,
            "learning_rate": 2.746478873239437e-05,
            "epoch": 4.507042253521127,
            "step": 320
        },
        {
            "loss": 0.7396,
            "grad_norm": 8.587728500366211,
            "learning_rate": 2.676056338028169e-05,
            "epoch": 4.647887323943662,
            "step": 330
        },
        {
            "loss": 0.8285,
            "grad_norm": 8.4628324508667,
            "learning_rate": 2.6056338028169013e-05,
            "epoch": 4.788732394366197,
            "step": 340
        },
        {
            "loss": 0.7445,
            "grad_norm": 14.041528701782227,
            "learning_rate": 2.535211267605634e-05,
            "epoch": 4.929577464788732,
            "step": 350
        },
        {
            "loss": 0.664,
            "grad_norm": 5.87201452255249,
            "learning_rate": 2.4647887323943664e-05,
            "epoch": 5.070422535211268,
            "step": 360
        },
        {
            "loss": 0.5067,
            "grad_norm": 10.788759231567383,
            "learning_rate": 2.3943661971830986e-05,
            "epoch": 5.211267605633803,
            "step": 370
        },
        {
            "loss": 0.5232,
            "grad_norm": 7.012488842010498,
            "learning_rate": 2.323943661971831e-05,
            "epoch": 5.352112676056338,
            "step": 380
        },
        {
            "loss": 0.5209,
            "grad_norm": 7.067287445068359,
            "learning_rate": 2.2535211267605634e-05,
            "epoch": 5.492957746478873,
            "step": 390
        },
        {
            "loss": 0.5193,
            "grad_norm": 10.486701965332031,
            "learning_rate": 2.1830985915492956e-05,
            "epoch": 5.633802816901408,
            "step": 400
        },
        {
            "loss": 0.518,
            "grad_norm": 8.49307632446289,
            "learning_rate": 2.112676056338028e-05,
            "epoch": 5.774647887323944,
            "step": 410
        },
        {
            "loss": 0.5616,
            "grad_norm": 13.104801177978516,
            "learning_rate": 2.0422535211267607e-05,
            "epoch": 5.915492957746479,
            "step": 420
        },
        {
            "loss": 0.5009,
            "grad_norm": 5.934049129486084,
            "learning_rate": 1.971830985915493e-05,
            "epoch": 6.056338028169014,
            "step": 430
        },
        {
            "loss": 0.3293,
            "grad_norm": 6.81897497177124,
            "learning_rate": 1.9014084507042255e-05,
            "epoch": 6.197183098591549,
            "step": 440
        },
        {
            "loss": 0.4494,
            "grad_norm": 8.095144271850586,
            "learning_rate": 1.830985915492958e-05,
            "epoch": 6.338028169014084,
            "step": 450
        },
        {
            "loss": 0.3411,
            "grad_norm": 12.16441535949707,
            "learning_rate": 1.7605633802816902e-05,
            "epoch": 6.47887323943662,
            "step": 460
        },
        {
            "loss": 0.4469,
            "grad_norm": 6.186120510101318,
            "learning_rate": 1.6901408450704224e-05,
            "epoch": 6.619718309859155,
            "step": 470
        },
        {
            "loss": 0.3855,
            "grad_norm": 11.119467735290527,
            "learning_rate": 1.619718309859155e-05,
            "epoch": 6.76056338028169,
            "step": 480
        },
        {
            "loss": 0.4436,
            "grad_norm": 6.551356792449951,
            "learning_rate": 1.5492957746478872e-05,
            "epoch": 6.901408450704225,
            "step": 490
        },
        {
            "loss": 0.3866,
            "grad_norm": 4.486627578735352,
            "learning_rate": 1.4788732394366198e-05,
            "epoch": 7.042253521126761,
            "step": 500
        },
        {
            "loss": 0.2913,
            "grad_norm": 6.422326564788818,
            "learning_rate": 1.4084507042253523e-05,
            "epoch": 7.183098591549296,
            "step": 510
        },
        {
            "loss": 0.362,
            "grad_norm": 4.92641544342041,
            "learning_rate": 1.3380281690140845e-05,
            "epoch": 7.323943661971831,
            "step": 520
        },
        {
            "loss": 0.2966,
            "grad_norm": 6.009878635406494,
            "learning_rate": 1.267605633802817e-05,
            "epoch": 7.464788732394366,
            "step": 530
        },
        {
            "loss": 0.3208,
            "grad_norm": 5.652812480926514,
            "learning_rate": 1.1971830985915493e-05,
            "epoch": 7.605633802816901,
            "step": 540
        },
        {
            "loss": 0.3242,
            "grad_norm": 7.107967853546143,
            "learning_rate": 1.1267605633802817e-05,
            "epoch": 7.746478873239437,
            "step": 550
        },
        {
            "loss": 0.3364,
            "grad_norm": 6.501546859741211,
            "learning_rate": 1.056338028169014e-05,
            "epoch": 7.887323943661972,
            "step": 560
        },
        {
            "loss": 0.2866,
            "grad_norm": 4.782270908355713,
            "learning_rate": 9.859154929577465e-06,
            "epoch": 8.028169014084508,
            "step": 570
        },
        {
            "loss": 0.2574,
            "grad_norm": 5.491822719573975,
            "learning_rate": 9.15492957746479e-06,
            "epoch": 8.169014084507042,
            "step": 580
        },
        {
            "loss": 0.2465,
            "grad_norm": 4.0202250480651855,
            "learning_rate": 8.450704225352112e-06,
            "epoch": 8.309859154929578,
            "step": 590
        },
        {
            "loss": 0.2793,
            "grad_norm": 6.404356002807617,
            "learning_rate": 7.746478873239436e-06,
            "epoch": 8.450704225352112,
            "step": 600
        },
        {
            "loss": 0.3116,
            "grad_norm": 4.467892646789551,
            "learning_rate": 7.042253521126762e-06,
            "epoch": 8.591549295774648,
            "step": 610
        },
        {
            "loss": 0.2803,
            "grad_norm": 8.141590118408203,
            "learning_rate": 6.338028169014085e-06,
            "epoch": 8.732394366197184,
            "step": 620
        },
        {
            "loss": 0.3222,
            "grad_norm": 5.973969459533691,
            "learning_rate": 5.6338028169014084e-06,
            "epoch": 8.873239436619718,
            "step": 630
        },
        {
            "loss": 0.2672,
            "grad_norm": 3.914694309234619,
            "learning_rate": 4.929577464788732e-06,
            "epoch": 9.014084507042254,
            "step": 640
        },
        {
            "loss": 0.2578,
            "grad_norm": 3.4964489936828613,
            "learning_rate": 4.225352112676056e-06,
            "epoch": 9.154929577464788,
            "step": 650
        },
        {
            "loss": 0.2611,
            "grad_norm": 4.2931599617004395,
            "learning_rate": 3.521126760563381e-06,
            "epoch": 9.295774647887324,
            "step": 660
        },
        {
            "loss": 0.2459,
            "grad_norm": 6.719724178314209,
            "learning_rate": 2.8169014084507042e-06,
            "epoch": 9.43661971830986,
            "step": 670
        },
        {
            "loss": 0.2331,
            "grad_norm": 2.5943868160247803,
            "learning_rate": 2.112676056338028e-06,
            "epoch": 9.577464788732394,
            "step": 680
        },
        {
            "loss": 0.2854,
            "grad_norm": 4.600313663482666,
            "learning_rate": 1.4084507042253521e-06,
            "epoch": 9.71830985915493,
            "step": 690
        },
        {
            "loss": 0.2387,
            "grad_norm": 3.987403392791748,
            "learning_rate": 7.042253521126761e-07,
            "epoch": 9.859154929577464,
            "step": 700
        },
        {
            "loss": 0.2564,
            "grad_norm": 3.1139235496520996,
            "learning_rate": 0.0,
            "epoch": 10.0,
            "step": 710
        },
        {
            "train_runtime": 6201.8925,
            "train_samples_per_second": 1.83,
            "train_steps_per_second": 0.114,
            "total_flos": 2.624438456549376e+17,
            "train_loss": 1.0979887223579514,
            "epoch": 10.0,
            "step": 710
        },
        {
            "eval_loss": 2.580629587173462,
            "eval_runtime": 52.2869,
            "eval_samples_per_second": 5.432,
            "eval_steps_per_second": 0.689,
            "epoch": 10.0,
            "step": 710
        }
    ],
    "perplexity": 13.205449524659944
}