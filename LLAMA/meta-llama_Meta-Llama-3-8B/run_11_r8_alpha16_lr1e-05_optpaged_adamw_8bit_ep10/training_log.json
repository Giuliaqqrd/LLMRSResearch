{
    "run_name": "run_11_r8_alpha16_lr1e-05_optpaged_adamw_8bit_ep10",
    "r": 8,
    "lora_alpha": 16,
    "optim": "paged_adamw_8bit",
    "learning_rate": 1e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.01,
    "num_train_epochs": 10,
    "train_log": [
        {
            "loss": 3.4886,
            "grad_norm": 1.1644680500030518,
            "learning_rate": 9.859154929577466e-06,
            "epoch": 0.14084507042253522,
            "step": 10
        },
        {
            "loss": 3.495,
            "grad_norm": 0.9079632759094238,
            "learning_rate": 9.71830985915493e-06,
            "epoch": 0.28169014084507044,
            "step": 20
        },
        {
            "loss": 3.7161,
            "grad_norm": 1.1289156675338745,
            "learning_rate": 9.577464788732394e-06,
            "epoch": 0.4225352112676056,
            "step": 30
        },
        {
            "loss": 3.2713,
            "grad_norm": 1.2566571235656738,
            "learning_rate": 9.43661971830986e-06,
            "epoch": 0.5633802816901409,
            "step": 40
        },
        {
            "loss": 3.3099,
            "grad_norm": 1.5527710914611816,
            "learning_rate": 9.295774647887325e-06,
            "epoch": 0.704225352112676,
            "step": 50
        },
        {
            "loss": 3.4494,
            "grad_norm": 1.5089882612228394,
            "learning_rate": 9.15492957746479e-06,
            "epoch": 0.8450704225352113,
            "step": 60
        },
        {
            "loss": 3.6248,
            "grad_norm": 2.0506274700164795,
            "learning_rate": 9.014084507042254e-06,
            "epoch": 0.9859154929577465,
            "step": 70
        },
        {
            "loss": 3.6417,
            "grad_norm": 2.2011921405792236,
            "learning_rate": 8.87323943661972e-06,
            "epoch": 1.1267605633802817,
            "step": 80
        },
        {
            "loss": 3.338,
            "grad_norm": 2.327756404876709,
            "learning_rate": 8.732394366197183e-06,
            "epoch": 1.267605633802817,
            "step": 90
        },
        {
            "loss": 3.2798,
            "grad_norm": 2.3982138633728027,
            "learning_rate": 8.591549295774648e-06,
            "epoch": 1.408450704225352,
            "step": 100
        },
        {
            "loss": 3.1989,
            "grad_norm": 2.189657211303711,
            "learning_rate": 8.450704225352114e-06,
            "epoch": 1.5492957746478875,
            "step": 110
        },
        {
            "loss": 3.2252,
            "grad_norm": 3.1369316577911377,
            "learning_rate": 8.309859154929578e-06,
            "epoch": 1.6901408450704225,
            "step": 120
        },
        {
            "loss": 2.9107,
            "grad_norm": 2.421037197113037,
            "learning_rate": 8.169014084507043e-06,
            "epoch": 1.8309859154929577,
            "step": 130
        },
        {
            "loss": 3.0423,
            "grad_norm": 2.6010751724243164,
            "learning_rate": 8.028169014084509e-06,
            "epoch": 1.971830985915493,
            "step": 140
        },
        {
            "loss": 3.2249,
            "grad_norm": 2.656053066253662,
            "learning_rate": 7.887323943661972e-06,
            "epoch": 2.112676056338028,
            "step": 150
        },
        {
            "loss": 3.0055,
            "grad_norm": 3.529568672180176,
            "learning_rate": 7.746478873239436e-06,
            "epoch": 2.2535211267605635,
            "step": 160
        },
        {
            "loss": 3.0899,
            "grad_norm": 3.181138515472412,
            "learning_rate": 7.6056338028169015e-06,
            "epoch": 2.3943661971830985,
            "step": 170
        },
        {
            "loss": 2.8176,
            "grad_norm": 2.7840607166290283,
            "learning_rate": 7.464788732394367e-06,
            "epoch": 2.535211267605634,
            "step": 180
        },
        {
            "loss": 2.9133,
            "grad_norm": 2.474364757537842,
            "learning_rate": 7.3239436619718316e-06,
            "epoch": 2.676056338028169,
            "step": 190
        },
        {
            "loss": 2.9291,
            "grad_norm": 3.4583187103271484,
            "learning_rate": 7.183098591549297e-06,
            "epoch": 2.816901408450704,
            "step": 200
        },
        {
            "loss": 2.9872,
            "grad_norm": 2.751265048980713,
            "learning_rate": 7.042253521126761e-06,
            "epoch": 2.9577464788732395,
            "step": 210
        },
        {
            "loss": 3.0581,
            "grad_norm": 4.181789398193359,
            "learning_rate": 6.901408450704225e-06,
            "epoch": 3.0985915492957745,
            "step": 220
        },
        {
            "loss": 2.817,
            "grad_norm": 3.061945915222168,
            "learning_rate": 6.760563380281691e-06,
            "epoch": 3.23943661971831,
            "step": 230
        },
        {
            "loss": 2.8134,
            "grad_norm": 2.6905229091644287,
            "learning_rate": 6.619718309859155e-06,
            "epoch": 3.380281690140845,
            "step": 240
        },
        {
            "loss": 2.7604,
            "grad_norm": 3.1137771606445312,
            "learning_rate": 6.478873239436621e-06,
            "epoch": 3.52112676056338,
            "step": 250
        },
        {
            "loss": 2.7651,
            "grad_norm": 3.648179769515991,
            "learning_rate": 6.3380281690140855e-06,
            "epoch": 3.6619718309859155,
            "step": 260
        },
        {
            "loss": 2.7475,
            "grad_norm": 3.3455841541290283,
            "learning_rate": 6.197183098591549e-06,
            "epoch": 3.802816901408451,
            "step": 270
        },
        {
            "loss": 2.7451,
            "grad_norm": 4.263491630554199,
            "learning_rate": 6.056338028169015e-06,
            "epoch": 3.943661971830986,
            "step": 280
        },
        {
            "loss": 2.8495,
            "grad_norm": 5.176492214202881,
            "learning_rate": 5.915492957746479e-06,
            "epoch": 4.084507042253521,
            "step": 290
        },
        {
            "loss": 2.7239,
            "grad_norm": 2.9610886573791504,
            "learning_rate": 5.774647887323944e-06,
            "epoch": 4.225352112676056,
            "step": 300
        },
        {
            "loss": 2.6368,
            "grad_norm": 4.290696620941162,
            "learning_rate": 5.633802816901409e-06,
            "epoch": 4.366197183098592,
            "step": 310
        },
        {
            "loss": 2.5521,
            "grad_norm": 4.652925491333008,
            "learning_rate": 5.492957746478874e-06,
            "epoch": 4.507042253521127,
            "step": 320
        },
        {
            "loss": 2.5225,
            "grad_norm": 4.160615921020508,
            "learning_rate": 5.352112676056338e-06,
            "epoch": 4.647887323943662,
            "step": 330
        },
        {
            "loss": 2.7528,
            "grad_norm": 3.623777151107788,
            "learning_rate": 5.211267605633803e-06,
            "epoch": 4.788732394366197,
            "step": 340
        },
        {
            "loss": 2.5916,
            "grad_norm": 4.658814430236816,
            "learning_rate": 5.070422535211268e-06,
            "epoch": 4.929577464788732,
            "step": 350
        },
        {
            "loss": 2.6591,
            "grad_norm": 3.728639602661133,
            "learning_rate": 4.929577464788733e-06,
            "epoch": 5.070422535211268,
            "step": 360
        },
        {
            "loss": 2.5625,
            "grad_norm": 4.105822563171387,
            "learning_rate": 4.788732394366197e-06,
            "epoch": 5.211267605633803,
            "step": 370
        },
        {
            "loss": 2.6031,
            "grad_norm": 3.778372287750244,
            "learning_rate": 4.647887323943662e-06,
            "epoch": 5.352112676056338,
            "step": 380
        },
        {
            "loss": 2.5353,
            "grad_norm": 3.8697941303253174,
            "learning_rate": 4.507042253521127e-06,
            "epoch": 5.492957746478873,
            "step": 390
        },
        {
            "loss": 2.5272,
            "grad_norm": 6.10545539855957,
            "learning_rate": 4.3661971830985915e-06,
            "epoch": 5.633802816901408,
            "step": 400
        },
        {
            "loss": 2.3428,
            "grad_norm": 4.1014723777771,
            "learning_rate": 4.225352112676057e-06,
            "epoch": 5.774647887323944,
            "step": 410
        },
        {
            "loss": 2.5586,
            "grad_norm": 6.364675045013428,
            "learning_rate": 4.0845070422535216e-06,
            "epoch": 5.915492957746479,
            "step": 420
        },
        {
            "loss": 2.6337,
            "grad_norm": 4.1099853515625,
            "learning_rate": 3.943661971830986e-06,
            "epoch": 6.056338028169014,
            "step": 430
        },
        {
            "loss": 2.3675,
            "grad_norm": 4.466509819030762,
            "learning_rate": 3.8028169014084508e-06,
            "epoch": 6.197183098591549,
            "step": 440
        },
        {
            "loss": 2.6143,
            "grad_norm": 4.503783226013184,
            "learning_rate": 3.6619718309859158e-06,
            "epoch": 6.338028169014084,
            "step": 450
        },
        {
            "loss": 2.255,
            "grad_norm": 4.874037742614746,
            "learning_rate": 3.5211267605633804e-06,
            "epoch": 6.47887323943662,
            "step": 460
        },
        {
            "loss": 2.4099,
            "grad_norm": 4.3296403884887695,
            "learning_rate": 3.3802816901408454e-06,
            "epoch": 6.619718309859155,
            "step": 470
        },
        {
            "loss": 2.4362,
            "grad_norm": 6.153205394744873,
            "learning_rate": 3.2394366197183104e-06,
            "epoch": 6.76056338028169,
            "step": 480
        },
        {
            "loss": 2.3556,
            "grad_norm": 6.673102855682373,
            "learning_rate": 3.0985915492957746e-06,
            "epoch": 6.901408450704225,
            "step": 490
        },
        {
            "loss": 2.4192,
            "grad_norm": 5.05598258972168,
            "learning_rate": 2.9577464788732396e-06,
            "epoch": 7.042253521126761,
            "step": 500
        },
        {
            "loss": 2.305,
            "grad_norm": 4.761646747589111,
            "learning_rate": 2.8169014084507046e-06,
            "epoch": 7.183098591549296,
            "step": 510
        },
        {
            "loss": 2.3528,
            "grad_norm": 4.340404987335205,
            "learning_rate": 2.676056338028169e-06,
            "epoch": 7.323943661971831,
            "step": 520
        },
        {
            "loss": 2.3718,
            "grad_norm": 6.693521976470947,
            "learning_rate": 2.535211267605634e-06,
            "epoch": 7.464788732394366,
            "step": 530
        },
        {
            "loss": 2.3275,
            "grad_norm": 4.255154132843018,
            "learning_rate": 2.3943661971830984e-06,
            "epoch": 7.605633802816901,
            "step": 540
        },
        {
            "loss": 2.3489,
            "grad_norm": 5.332186698913574,
            "learning_rate": 2.2535211267605635e-06,
            "epoch": 7.746478873239437,
            "step": 550
        },
        {
            "loss": 2.3913,
            "grad_norm": 5.333337306976318,
            "learning_rate": 2.1126760563380285e-06,
            "epoch": 7.887323943661972,
            "step": 560
        },
        {
            "loss": 2.1809,
            "grad_norm": 8.053915023803711,
            "learning_rate": 1.971830985915493e-06,
            "epoch": 8.028169014084508,
            "step": 570
        },
        {
            "loss": 2.186,
            "grad_norm": 6.597115993499756,
            "learning_rate": 1.8309859154929579e-06,
            "epoch": 8.169014084507042,
            "step": 580
        },
        {
            "loss": 2.2348,
            "grad_norm": 4.704248428344727,
            "learning_rate": 1.6901408450704227e-06,
            "epoch": 8.309859154929578,
            "step": 590
        },
        {
            "loss": 2.3825,
            "grad_norm": 6.54629373550415,
            "learning_rate": 1.5492957746478873e-06,
            "epoch": 8.450704225352112,
            "step": 600
        },
        {
            "loss": 2.3943,
            "grad_norm": 4.975762844085693,
            "learning_rate": 1.4084507042253523e-06,
            "epoch": 8.591549295774648,
            "step": 610
        },
        {
            "loss": 2.3546,
            "grad_norm": 5.779988765716553,
            "learning_rate": 1.267605633802817e-06,
            "epoch": 8.732394366197184,
            "step": 620
        },
        {
            "loss": 2.2741,
            "grad_norm": 6.022773265838623,
            "learning_rate": 1.1267605633802817e-06,
            "epoch": 8.873239436619718,
            "step": 630
        },
        {
            "loss": 2.2323,
            "grad_norm": 5.421720504760742,
            "learning_rate": 9.859154929577465e-07,
            "epoch": 9.014084507042254,
            "step": 640
        },
        {
            "loss": 2.352,
            "grad_norm": 4.411355018615723,
            "learning_rate": 8.450704225352114e-07,
            "epoch": 9.154929577464788,
            "step": 650
        },
        {
            "loss": 2.1296,
            "grad_norm": 6.082650661468506,
            "learning_rate": 7.042253521126762e-07,
            "epoch": 9.295774647887324,
            "step": 660
        },
        {
            "loss": 2.3068,
            "grad_norm": 5.068149566650391,
            "learning_rate": 5.633802816901409e-07,
            "epoch": 9.43661971830986,
            "step": 670
        },
        {
            "loss": 2.136,
            "grad_norm": 4.31795597076416,
            "learning_rate": 4.225352112676057e-07,
            "epoch": 9.577464788732394,
            "step": 680
        },
        {
            "loss": 2.3967,
            "grad_norm": 5.83748722076416,
            "learning_rate": 2.8169014084507043e-07,
            "epoch": 9.71830985915493,
            "step": 690
        },
        {
            "loss": 2.1726,
            "grad_norm": 5.537035942077637,
            "learning_rate": 1.4084507042253522e-07,
            "epoch": 9.859154929577464,
            "step": 700
        },
        {
            "loss": 2.2147,
            "grad_norm": 5.058619022369385,
            "learning_rate": 0.0,
            "epoch": 10.0,
            "step": 710
        },
        {
            "train_runtime": 6202.161,
            "train_samples_per_second": 1.83,
            "train_steps_per_second": 0.114,
            "total_flos": 2.624438456549376e+17,
            "train_loss": 2.698872184753418,
            "epoch": 10.0,
            "step": 710
        },
        {
            "eval_loss": 2.671704053878784,
            "eval_runtime": 52.2944,
            "eval_samples_per_second": 5.431,
            "eval_steps_per_second": 0.688,
            "epoch": 10.0,
            "step": 710
        }
    ],
    "perplexity": 14.464596655617735
}