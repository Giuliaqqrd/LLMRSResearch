services:
  tgi:
    image: ghcr.io/huggingface/text-generation-inference:2.4.1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    environment:
      MODEL_ID: meta-llama/Llama-3.1-8B-Instruct #model ID
    ports:
      - "40900:80" # Map port 40900 on the host to port 80 in the container
    volumes:
      - /mnt/storage/huggingface:/data # Share the volume with the container
      - /home/ubuntu/.cache/huggingface:/root/.cache/huggingface
    shm_size: '40g' # Set shared memory size
    command: >
      --trust-remote-code
#      --model-id meta-llama/Llama-3.2-1B
#      --quantize awq
#      --num-shard 4
#      --usage-stats off
#      --cuda-memory-fraction 0.2